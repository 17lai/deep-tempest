23-08-02 02:14:07.228 :   task: drunet
  model: plain
  gpu_ids: [0]
  scale: 0
  n_channels: 1
  n_channels_datasetload: 3
  path:[
    root: optuna_hparams
    pretrained_netG: denoising/drunet/models/520_G.pth
    task: optuna_hparams/drunet
    log: optuna_hparams/drunet
    options: optuna_hparams/drunet/options
    models: optuna_hparams/drunet/models
    images: optuna_hparams/drunet/images
  ]
  optuna:[
    n_trials: 20
    trial_epochs: 10
    metric: edgeJaccard
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: ffdnet
      dataroot_H: trainsets/ground-truth
      dataroot_L: trainsets/simulations
      sigma: [0, 15]
      num_patches_per_image: 21
      H_size: 256
      dataloader_shuffle: True
      dataloader_num_workers: 8
      dataloader_batch_size: 28
      phase: train
      scale: 0
      n_channels: 1
    ]
    test:[
      name: test_dataset
      dataset_type: ffdnet
      dataroot_H: testsets/ground-truth
      dataroot_L: testsets/simulations
      sigma_test: 10
      phase: test
      scale: 0
      n_channels: 1
    ]
  ]
  netG:[
    net_type: drunet
    in_nc: 2
    out_nc: 1
    nc: [64, 128, 256, 512]
    nb: 4
    gc: 32
    ng: 2
    reduction: 16
    act_mode: R
    upsample_mode: convtranspose
    downsample_mode: strideconv
    bias: False
    init_type: orthogonal
    init_bn_type: uniform
    init_gain: 0.2
    scale: 0
  ]
  train:[
    epochs: 1000
    G_lossfn_type: tv
    G_lossfn_weight: 1.0
    G_tvloss_weight: 0.1
    G_optimizer_type: adam
    G_optimizer_lr: 0.0001
    G_optimizer_clipgrad: None
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [1600, 3200, 4800, 6400, 8000, 9600, 11200, 12800, 14400]
    G_scheduler_gamma: 0.1
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    checkpoint_test: 1600
    checkpoint_save: 3999
    checkpoint_print: 16
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
    G_optimizer_wd: 0
    G_optimizer_reuse: False
    G_param_strict: True
    E_param_strict: True
    E_decay: 0
  ]
  opt_path: options/optuna_options.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  dist: False
  num_gpu: 1
  rank: 0
  world_size: 1

23-08-02 02:14:07.228 : Loading train and val datasets
23-08-02 02:14:07.228 : Random seed: 6168
23-08-02 02:14:07.383 : Datasets loaded.
23-08-02 02:14:07.384 : Trial number 0 with parameters:
lr = 0.0322545143172991
tv_weight = 1.7315238828664557e-06
23-08-02 02:20:36.463 : 
epoch:1/10
--------------
train loss: 2.060e-02, train edgeJaccard: 0.393
val loss: 1.133e-02, val edgeJaccard: 0.478
--------------
23-08-02 02:27:05.742 : 
epoch:2/10
--------------
train loss: 2.000e-02, train edgeJaccard: 0.414
val loss: 1.095e-02, val edgeJaccard: 0.480
--------------
23-08-02 02:33:35.306 : 
epoch:3/10
--------------
train loss: 1.852e-02, train edgeJaccard: 0.400
val loss: 1.021e-02, val edgeJaccard: 0.478
--------------
23-08-02 02:40:05.198 : 
epoch:4/10
--------------
train loss: 1.718e-02, train edgeJaccard: 0.395
val loss: 1.117e-02, val edgeJaccard: 0.483
--------------
23-08-02 02:46:34.673 : 
epoch:5/10
--------------
train loss: 1.649e-02, train edgeJaccard: 0.422
val loss: 9.551e-03, val edgeJaccard: 0.495
--------------
23-08-02 02:53:04.727 : 
epoch:6/10
--------------
train loss: 1.423e-02, train edgeJaccard: 0.400
val loss: 9.336e-03, val edgeJaccard: 0.499
--------------
23-08-02 02:59:34.603 : 
epoch:7/10
--------------
train loss: 1.437e-02, train edgeJaccard: 0.405
val loss: 9.043e-03, val edgeJaccard: 0.499
--------------
23-08-02 03:06:05.119 : 
epoch:8/10
--------------
train loss: 1.388e-02, train edgeJaccard: 0.434
val loss: 9.622e-03, val edgeJaccard: 0.500
--------------
23-08-02 03:12:35.219 : 
epoch:9/10
--------------
train loss: 1.321e-02, train edgeJaccard: 0.430
val loss: 9.594e-03, val edgeJaccard: 0.502
--------------
23-08-02 03:19:05.279 : 
epoch:10/10
--------------
train loss: 1.313e-02, train edgeJaccard: 0.476
val loss: 9.273e-03, val edgeJaccard: 0.503
--------------
23-08-02 03:19:05.280 : Trial 0: training completed in 1hs 64min 57s
23-08-02 03:19:05.281 : Trial number 1 with parameters:
lr = 0.00034100242346060173
tv_weight = 0.00015690489872538057
23-08-02 03:25:35.657 : 
epoch:1/10
--------------
train loss: 2.073e-02, train edgeJaccard: 0.413
val loss: 1.197e-02, val edgeJaccard: 0.486
--------------
23-08-02 03:32:05.899 : 
epoch:2/10
--------------
train loss: 1.822e-02, train edgeJaccard: 0.417
val loss: 1.329e-02, val edgeJaccard: 0.466
--------------
23-08-02 03:38:35.938 : 
epoch:3/10
--------------
train loss: 1.868e-02, train edgeJaccard: 0.419
val loss: 1.283e-02, val edgeJaccard: 0.470
--------------
23-08-02 03:45:06.164 : 
epoch:4/10
--------------
train loss: 1.779e-02, train edgeJaccard: 0.400
val loss: 1.013e-02, val edgeJaccard: 0.473
--------------
23-08-02 03:51:36.808 : 
epoch:5/10
--------------
train loss: 1.653e-02, train edgeJaccard: 0.405
val loss: 1.066e-02, val edgeJaccard: 0.495
--------------
23-08-02 03:58:07.247 : 
epoch:6/10
--------------
train loss: 1.422e-02, train edgeJaccard: 0.429
val loss: 1.084e-02, val edgeJaccard: 0.492
--------------
23-08-02 04:04:38.196 : 
epoch:7/10
--------------
train loss: 1.406e-02, train edgeJaccard: 0.447
val loss: 9.998e-03, val edgeJaccard: 0.496
--------------
23-08-02 04:11:08.249 : 
epoch:8/10
--------------
train loss: 1.286e-02, train edgeJaccard: 0.471
val loss: 1.031e-02, val edgeJaccard: 0.497
--------------
23-08-02 04:17:38.111 : 
epoch:9/10
--------------
train loss: 1.331e-02, train edgeJaccard: 0.405
val loss: 1.049e-02, val edgeJaccard: 0.495
--------------
23-08-02 04:24:08.638 : 
epoch:10/10
--------------
train loss: 1.298e-02, train edgeJaccard: 0.436
val loss: 1.047e-02, val edgeJaccard: 0.496
--------------
23-08-02 04:24:08.638 : Trial 1: training completed in 1hs 65min 3s
23-08-02 04:24:08.641 : Trial number 2 with parameters:
lr = 0.021755518311190474
tv_weight = 1.6407778211763104e-05
23-08-02 04:30:39.120 : 
epoch:1/10
--------------
train loss: 2.012e-02, train edgeJaccard: 0.408
val loss: 1.079e-02, val edgeJaccard: 0.467
--------------
23-08-02 04:37:08.868 : 
epoch:2/10
--------------
train loss: 1.867e-02, train edgeJaccard: 0.411
val loss: 1.314e-02, val edgeJaccard: 0.468
--------------
23-08-02 04:43:38.432 : 
epoch:3/10
--------------
train loss: 1.824e-02, train edgeJaccard: 0.409
val loss: 1.274e-02, val edgeJaccard: 0.468
--------------
23-08-02 04:50:07.966 : 
epoch:4/10
--------------
train loss: 1.742e-02, train edgeJaccard: 0.393
val loss: 1.158e-02, val edgeJaccard: 0.483
--------------
23-08-02 04:56:38.806 : 
epoch:5/10
--------------
train loss: 1.644e-02, train edgeJaccard: 0.432
val loss: 1.200e-02, val edgeJaccard: 0.491
--------------
23-08-02 05:03:09.439 : 
epoch:6/10
--------------
train loss: 1.446e-02, train edgeJaccard: 0.441
val loss: 1.117e-02, val edgeJaccard: 0.497
--------------
23-08-02 05:09:39.186 : 
epoch:7/10
--------------
train loss: 1.443e-02, train edgeJaccard: 0.419
val loss: 1.061e-02, val edgeJaccard: 0.495
--------------
23-08-02 05:16:08.895 : 
epoch:8/10
--------------
train loss: 1.327e-02, train edgeJaccard: 0.408
val loss: 1.089e-02, val edgeJaccard: 0.498
--------------
23-08-02 05:22:38.731 : 
epoch:9/10
--------------
train loss: 1.313e-02, train edgeJaccard: 0.450
val loss: 1.053e-02, val edgeJaccard: 0.500
--------------
23-08-02 05:29:08.361 : 
epoch:10/10
--------------
train loss: 1.290e-02, train edgeJaccard: 0.415
val loss: 1.040e-02, val edgeJaccard: 0.502
--------------
23-08-02 05:29:08.361 : Trial 2: training completed in 1hs 64min 59s
23-08-02 05:29:08.362 : Trial number 3 with parameters:
lr = 0.0049452164216553475
tv_weight = 1.9077419418647102e-07
23-08-02 05:35:39.366 : 
epoch:1/10
--------------
train loss: 2.080e-02, train edgeJaccard: 0.388
val loss: 1.083e-02, val edgeJaccard: 0.482
--------------
23-08-02 05:42:09.467 : 
epoch:2/10
--------------
train loss: 1.944e-02, train edgeJaccard: 0.384
val loss: 1.127e-02, val edgeJaccard: 0.484
--------------
23-08-02 05:48:39.602 : 
epoch:3/10
--------------
train loss: 1.801e-02, train edgeJaccard: 0.413
val loss: 1.099e-02, val edgeJaccard: 0.490
--------------
23-08-02 05:55:09.796 : 
epoch:4/10
--------------
train loss: 1.720e-02, train edgeJaccard: 0.405
val loss: 1.206e-02, val edgeJaccard: 0.471
--------------
23-08-02 06:01:39.442 : 
epoch:5/10
--------------
train loss: 1.649e-02, train edgeJaccard: 0.400
val loss: 1.131e-02, val edgeJaccard: 0.487
--------------
23-08-02 06:08:09.068 : 
epoch:6/10
--------------
train loss: 1.470e-02, train edgeJaccard: 0.406
val loss: 1.021e-02, val edgeJaccard: 0.491
--------------
23-08-02 06:08:09.068 : Trial number 3 pruned. Wasted 0hs 39min 0s on training ¯\_(ツ)_/¯
23-08-02 06:08:09.069 : Trial number 4 with parameters:
lr = 0.00017565661862233974
tv_weight = 1.0442152985670772e-05
23-08-02 06:14:39.579 : 
epoch:1/10
--------------
train loss: 2.038e-02, train edgeJaccard: 0.389
val loss: 1.003e-02, val edgeJaccard: 0.469
--------------
23-08-02 06:21:10.709 : 
epoch:2/10
--------------
train loss: 1.916e-02, train edgeJaccard: 0.412
val loss: 1.317e-02, val edgeJaccard: 0.474
--------------
23-08-02 06:27:40.834 : 
epoch:3/10
--------------
train loss: 1.847e-02, train edgeJaccard: 0.398
val loss: 1.214e-02, val edgeJaccard: 0.474
--------------
23-08-02 06:34:11.518 : 
epoch:4/10
--------------
train loss: 1.743e-02, train edgeJaccard: 0.393
val loss: 1.119e-02, val edgeJaccard: 0.479
--------------
23-08-02 06:40:41.286 : 
epoch:5/10
--------------
train loss: 1.682e-02, train edgeJaccard: 0.426
val loss: 1.085e-02, val edgeJaccard: 0.493
--------------
23-08-02 06:47:10.752 : 
epoch:6/10
--------------
train loss: 1.432e-02, train edgeJaccard: 0.399
val loss: 9.922e-03, val edgeJaccard: 0.495
--------------
23-08-02 06:47:10.752 : Trial number 4 pruned. Wasted 0hs 39min 1s on training ¯\_(ツ)_/¯
23-08-02 06:47:10.753 : Trial number 5 with parameters:
lr = 0.0015531666937886681
tv_weight = 3.814259574126671e-06
23-08-02 06:53:41.164 : 
epoch:1/10
--------------
train loss: 2.074e-02, train edgeJaccard: 0.408
val loss: 1.216e-02, val edgeJaccard: 0.464
--------------
23-08-02 07:00:11.009 : 
epoch:2/10
--------------
train loss: 1.925e-02, train edgeJaccard: 0.408
val loss: 1.117e-02, val edgeJaccard: 0.468
--------------
23-08-02 07:06:40.676 : 
epoch:3/10
--------------
train loss: 1.842e-02, train edgeJaccard: 0.401
val loss: 1.064e-02, val edgeJaccard: 0.475
--------------
23-08-02 07:13:10.359 : 
epoch:4/10
--------------
train loss: 1.803e-02, train edgeJaccard: 0.419
val loss: 1.237e-02, val edgeJaccard: 0.456
--------------
23-08-02 07:19:40.696 : 
epoch:5/10
--------------
train loss: 1.734e-02, train edgeJaccard: 0.407
val loss: 1.027e-02, val edgeJaccard: 0.488
--------------
23-08-02 07:26:10.352 : 
epoch:6/10
--------------
train loss: 1.458e-02, train edgeJaccard: 0.408
val loss: 9.915e-03, val edgeJaccard: 0.494
--------------
23-08-02 07:26:10.352 : Trial number 5 pruned. Wasted 0hs 38min 59s on training ¯\_(ツ)_/¯
23-08-02 07:26:10.353 : Trial number 6 with parameters:
lr = 5.2199993790192106e-05
tv_weight = 0.00442174180697455
23-08-02 07:32:41.473 : 
epoch:1/10
--------------
train loss: 2.123e-02, train edgeJaccard: 0.400
val loss: 1.190e-02, val edgeJaccard: 0.482
--------------
23-08-02 07:39:10.871 : 
epoch:2/10
--------------
train loss: 1.852e-02, train edgeJaccard: 0.421
val loss: 1.210e-02, val edgeJaccard: 0.477
--------------
23-08-02 07:45:41.438 : 
epoch:3/10
--------------
train loss: 1.840e-02, train edgeJaccard: 0.423
val loss: 9.816e-03, val edgeJaccard: 0.483
--------------
23-08-02 07:52:11.545 : 
epoch:4/10
--------------
train loss: 1.798e-02, train edgeJaccard: 0.400
val loss: 1.072e-02, val edgeJaccard: 0.476
--------------
23-08-02 07:58:41.876 : 
epoch:5/10
--------------
train loss: 1.638e-02, train edgeJaccard: 0.414
val loss: 1.013e-02, val edgeJaccard: 0.492
--------------
23-08-02 08:05:12.248 : 
epoch:6/10
--------------
train loss: 1.510e-02, train edgeJaccard: 0.429
val loss: 9.833e-03, val edgeJaccard: 0.496
--------------
23-08-02 08:05:12.249 : Trial number 6 pruned. Wasted 0hs 39min 1s on training ¯\_(ツ)_/¯
23-08-02 08:05:12.249 : Trial number 7 with parameters:
lr = 0.00015338998362465905
tv_weight = 0.007474438646008824
23-08-02 08:11:42.686 : 
epoch:1/10
--------------
train loss: 2.084e-02, train edgeJaccard: 0.404
val loss: 1.176e-02, val edgeJaccard: 0.485
--------------
23-08-02 08:18:13.167 : 
epoch:2/10
--------------
train loss: 1.971e-02, train edgeJaccard: 0.382
val loss: 1.244e-02, val edgeJaccard: 0.489
--------------
23-08-02 08:24:43.372 : 
epoch:3/10
--------------
train loss: 1.796e-02, train edgeJaccard: 0.399
val loss: 1.038e-02, val edgeJaccard: 0.484
--------------
23-08-02 08:31:13.066 : 
epoch:4/10
--------------
train loss: 1.817e-02, train edgeJaccard: 0.398
val loss: 1.211e-02, val edgeJaccard: 0.477
--------------
23-08-02 08:37:42.416 : 
epoch:5/10
--------------
train loss: 1.700e-02, train edgeJaccard: 0.438
val loss: 1.009e-02, val edgeJaccard: 0.491
--------------
23-08-02 08:44:12.636 : 
epoch:6/10
--------------
train loss: 1.516e-02, train edgeJaccard: 0.386
val loss: 1.061e-02, val edgeJaccard: 0.494
--------------
23-08-02 08:44:12.636 : Trial number 7 pruned. Wasted 0hs 38min 60s on training ¯\_(ツ)_/¯
23-08-02 08:44:12.637 : Trial number 8 with parameters:
lr = 0.020448689376485044
tv_weight = 3.261055730584879e-05
23-08-02 08:50:43.755 : 
epoch:1/10
--------------
train loss: 2.107e-02, train edgeJaccard: 0.433
val loss: 1.196e-02, val edgeJaccard: 0.464
--------------
23-08-02 08:57:13.998 : 
epoch:2/10
--------------
train loss: 1.872e-02, train edgeJaccard: 0.392
val loss: 1.009e-02, val edgeJaccard: 0.479
--------------
23-08-02 09:03:44.930 : 
epoch:3/10
--------------
train loss: 1.814e-02, train edgeJaccard: 0.407
val loss: 1.031e-02, val edgeJaccard: 0.472
--------------
23-08-02 09:10:15.343 : 
epoch:4/10
--------------
train loss: 1.721e-02, train edgeJaccard: 0.395
val loss: 1.079e-02, val edgeJaccard: 0.472
--------------
23-08-02 09:16:45.883 : 
epoch:5/10
--------------
train loss: 1.658e-02, train edgeJaccard: 0.419
val loss: 1.062e-02, val edgeJaccard: 0.488
--------------
23-08-02 09:23:16.912 : 
epoch:6/10
--------------
train loss: 1.451e-02, train edgeJaccard: 0.423
val loss: 1.001e-02, val edgeJaccard: 0.495
--------------
23-08-02 09:23:16.912 : Trial number 8 pruned. Wasted 0hs 39min 4s on training ¯\_(ツ)_/¯
23-08-02 09:23:16.913 : Trial number 9 with parameters:
lr = 5.026142929216303e-05
tv_weight = 3.086237692902846e-07
23-08-02 09:29:47.853 : 
epoch:1/10
--------------
train loss: 2.034e-02, train edgeJaccard: 0.371
val loss: 1.246e-02, val edgeJaccard: 0.473
--------------
23-08-02 09:36:17.544 : 
epoch:2/10
--------------
train loss: 1.854e-02, train edgeJaccard: 0.400
val loss: 1.048e-02, val edgeJaccard: 0.478
--------------
23-08-02 09:42:47.851 : 
epoch:3/10
--------------
train loss: 1.995e-02, train edgeJaccard: 0.384
val loss: 1.090e-02, val edgeJaccard: 0.468
--------------
23-08-02 09:49:18.215 : 
epoch:4/10
--------------
train loss: 1.767e-02, train edgeJaccard: 0.378
val loss: 1.171e-02, val edgeJaccard: 0.469
--------------
23-08-02 09:55:48.280 : 
epoch:5/10
--------------
train loss: 1.672e-02, train edgeJaccard: 0.420
val loss: 1.014e-02, val edgeJaccard: 0.488
--------------
23-08-02 10:02:18.982 : 
epoch:6/10
--------------
train loss: 1.461e-02, train edgeJaccard: 0.436
val loss: 9.844e-03, val edgeJaccard: 0.490
--------------
23-08-02 10:02:18.983 : Trial number 9 pruned. Wasted 0hs 39min 1s on training ¯\_(ツ)_/¯
23-08-02 10:02:18.989 : Trial number 10 with parameters:
lr = 0.09526122782984511
tv_weight = 1.1620921577260407e-06
23-08-02 10:08:50.138 : 
epoch:1/10
--------------
train loss: 2.157e-02, train edgeJaccard: 0.385
val loss: 1.072e-02, val edgeJaccard: 0.483
--------------
23-08-02 10:15:20.118 : 
epoch:2/10
--------------
train loss: 1.891e-02, train edgeJaccard: 0.394
val loss: 1.120e-02, val edgeJaccard: 0.483
--------------
23-08-02 10:21:50.784 : 
epoch:3/10
--------------
train loss: 1.759e-02, train edgeJaccard: 0.419
val loss: 1.303e-02, val edgeJaccard: 0.469
--------------
23-08-02 10:28:20.945 : 
epoch:4/10
--------------
train loss: 1.797e-02, train edgeJaccard: 0.400
val loss: 1.067e-02, val edgeJaccard: 0.480
--------------
23-08-02 10:34:51.347 : 
epoch:5/10
--------------
train loss: 1.595e-02, train edgeJaccard: 0.417
val loss: 1.056e-02, val edgeJaccard: 0.488
--------------
23-08-02 10:41:22.309 : 
epoch:6/10
--------------
train loss: 1.425e-02, train edgeJaccard: 0.423
val loss: 1.042e-02, val edgeJaccard: 0.495
--------------
23-08-02 10:41:22.310 : Trial number 10 pruned. Wasted 0hs 39min 3s on training ¯\_(ツ)_/¯
23-08-02 10:41:22.316 : Trial number 11 with parameters:
lr = 0.017588424369255864
tv_weight = 1.9588113584979073e-06
23-08-02 10:47:54.048 : 
epoch:1/10
--------------
train loss: 2.051e-02, train edgeJaccard: 0.391
val loss: 9.966e-03, val edgeJaccard: 0.480
--------------
23-08-02 10:54:26.031 : 
epoch:2/10
--------------
train loss: 1.907e-02, train edgeJaccard: 0.397
val loss: 1.178e-02, val edgeJaccard: 0.477
--------------
23-08-02 11:00:57.625 : 
epoch:3/10
--------------
train loss: 1.823e-02, train edgeJaccard: 0.396
val loss: 1.228e-02, val edgeJaccard: 0.468
--------------
23-08-02 11:07:28.659 : 
epoch:4/10
--------------
train loss: 1.749e-02, train edgeJaccard: 0.417
val loss: 1.244e-02, val edgeJaccard: 0.472
--------------
23-08-02 11:13:59.845 : 
epoch:5/10
--------------
train loss: 1.607e-02, train edgeJaccard: 0.395
val loss: 1.034e-02, val edgeJaccard: 0.491
--------------
23-08-02 11:20:30.796 : 
epoch:6/10
--------------
train loss: 1.429e-02, train edgeJaccard: 0.436
val loss: 1.001e-02, val edgeJaccard: 0.496
--------------
23-08-02 11:20:30.797 : Trial number 11 pruned. Wasted 0hs 39min 8s on training ¯\_(ツ)_/¯
23-08-02 11:20:30.803 : Trial number 12 with parameters:
lr = 0.09167945621455428
tv_weight = 2.967220156352461e-05
23-08-02 11:27:01.496 : 
epoch:1/10
--------------
train loss: 2.027e-02, train edgeJaccard: 0.394
val loss: 1.058e-02, val edgeJaccard: 0.483
--------------
23-08-02 11:33:31.755 : 
epoch:2/10
--------------
train loss: 1.900e-02, train edgeJaccard: 0.384
val loss: 1.081e-02, val edgeJaccard: 0.486
--------------
23-08-02 11:40:01.920 : 
epoch:3/10
--------------
train loss: 1.842e-02, train edgeJaccard: 0.390
val loss: 1.029e-02, val edgeJaccard: 0.481
--------------
23-08-02 11:46:32.055 : 
epoch:4/10
--------------
train loss: 1.707e-02, train edgeJaccard: 0.423
val loss: 1.106e-02, val edgeJaccard: 0.477
--------------
23-08-02 11:53:02.272 : 
epoch:5/10
--------------
train loss: 1.633e-02, train edgeJaccard: 0.434
val loss: 1.037e-02, val edgeJaccard: 0.490
--------------
23-08-02 11:59:32.879 : 
epoch:6/10
--------------
train loss: 1.454e-02, train edgeJaccard: 0.414
val loss: 9.809e-03, val edgeJaccard: 0.496
--------------
23-08-02 11:59:32.880 : Trial number 12 pruned. Wasted 0hs 39min 1s on training ¯\_(ツ)_/¯
23-08-02 11:59:32.891 : Trial number 13 with parameters:
lr = 0.005598665701736683
tv_weight = 1.184813868292755e-07
23-08-02 12:06:04.262 : 
epoch:1/10
--------------
train loss: 2.118e-02, train edgeJaccard: 0.410
val loss: 1.009e-02, val edgeJaccard: 0.482
--------------
23-08-02 12:12:35.663 : 
epoch:2/10
--------------
train loss: 1.860e-02, train edgeJaccard: 0.407
val loss: 9.558e-03, val edgeJaccard: 0.487
--------------
23-08-02 12:19:07.988 : 
epoch:3/10
--------------
train loss: 1.807e-02, train edgeJaccard: 0.401
val loss: 1.229e-02, val edgeJaccard: 0.467
--------------
23-08-02 12:25:40.138 : 
epoch:4/10
--------------
train loss: 1.769e-02, train edgeJaccard: 0.410
val loss: 1.339e-02, val edgeJaccard: 0.457
--------------
23-08-02 12:32:12.519 : 
epoch:5/10
--------------
train loss: 1.701e-02, train edgeJaccard: 0.414
val loss: 1.072e-02, val edgeJaccard: 0.485
--------------
23-08-02 12:38:46.501 : 
epoch:6/10
--------------
train loss: 1.439e-02, train edgeJaccard: 0.434
val loss: 1.053e-02, val edgeJaccard: 0.489
--------------
23-08-02 12:38:46.502 : Trial number 13 pruned. Wasted 0hs 39min 13s on training ¯\_(ツ)_/¯
23-08-02 12:38:46.508 : Trial number 14 with parameters:
lr = 1.4346767310602998e-05
tv_weight = 8.219454182127948e-07
23-08-02 12:45:19.370 : 
epoch:1/10
--------------
train loss: 2.056e-02, train edgeJaccard: 0.435
val loss: 1.134e-02, val edgeJaccard: 0.472
--------------
23-08-02 12:51:51.828 : 
epoch:2/10
--------------
train loss: 1.877e-02, train edgeJaccard: 0.422
val loss: 1.034e-02, val edgeJaccard: 0.487
--------------
23-08-02 12:58:22.772 : 
epoch:3/10
--------------
train loss: 1.786e-02, train edgeJaccard: 0.417
val loss: 1.013e-02, val edgeJaccard: 0.476
--------------
23-08-02 13:04:55.137 : 
epoch:4/10
--------------
train loss: 1.788e-02, train edgeJaccard: 0.402
val loss: 9.969e-03, val edgeJaccard: 0.490
--------------
23-08-02 13:11:27.218 : 
epoch:5/10
--------------
train loss: 1.677e-02, train edgeJaccard: 0.413
val loss: 1.078e-02, val edgeJaccard: 0.487
--------------
23-08-02 13:17:59.397 : 
epoch:6/10
--------------
train loss: 1.453e-02, train edgeJaccard: 0.436
val loss: 9.782e-03, val edgeJaccard: 0.493
--------------
23-08-02 13:17:59.398 : Trial number 14 pruned. Wasted 0hs 39min 12s on training ¯\_(ツ)_/¯
23-08-02 13:17:59.404 : Trial number 15 with parameters:
lr = 0.021690485963988548
tv_weight = 1.100158084944072e-05
23-08-02 13:24:32.636 : 
epoch:1/10
--------------
train loss: 2.074e-02, train edgeJaccard: 0.405
val loss: 1.368e-02, val edgeJaccard: 0.475
--------------
23-08-02 13:31:04.425 : 
epoch:2/10
--------------
train loss: 1.842e-02, train edgeJaccard: 0.414
val loss: 1.240e-02, val edgeJaccard: 0.468
--------------
23-08-02 13:37:40.199 : 
epoch:3/10
--------------
train loss: 1.841e-02, train edgeJaccard: 0.405
val loss: 1.057e-02, val edgeJaccard: 0.475
--------------
23-08-02 13:44:15.387 : 
epoch:4/10
--------------
train loss: 1.745e-02, train edgeJaccard: 0.415
val loss: 1.223e-02, val edgeJaccard: 0.489
--------------
23-08-02 13:50:47.983 : 
epoch:5/10
--------------
train loss: 1.629e-02, train edgeJaccard: 0.397
val loss: 9.649e-03, val edgeJaccard: 0.493
--------------
23-08-02 13:57:19.544 : 
epoch:6/10
--------------
train loss: 1.401e-02, train edgeJaccard: 0.413
val loss: 1.005e-02, val edgeJaccard: 0.497
--------------
23-08-02 13:57:19.544 : Trial number 15 pruned. Wasted 0hs 39min 19s on training ¯\_(ツ)_/¯
23-08-02 13:57:19.551 : Trial number 16 with parameters:
lr = 0.0016528675287317098
tv_weight = 5.12726177469224e-07
23-08-02 14:03:51.899 : 
epoch:1/10
--------------
train loss: 2.050e-02, train edgeJaccard: 0.405
val loss: 1.181e-02, val edgeJaccard: 0.491
--------------
23-08-02 14:10:24.077 : 
epoch:2/10
--------------
train loss: 1.881e-02, train edgeJaccard: 0.384
val loss: 9.618e-03, val edgeJaccard: 0.486
--------------
23-08-02 14:16:56.464 : 
epoch:3/10
--------------
train loss: 1.828e-02, train edgeJaccard: 0.411
val loss: 1.337e-02, val edgeJaccard: 0.478
--------------
23-08-02 14:23:28.875 : 
epoch:4/10
--------------
train loss: 1.809e-02, train edgeJaccard: 0.395
val loss: 1.180e-02, val edgeJaccard: 0.479
--------------
23-08-02 14:30:00.622 : 
epoch:5/10
--------------
train loss: 1.646e-02, train edgeJaccard: 0.397
val loss: 1.021e-02, val edgeJaccard: 0.491
--------------
23-08-02 14:36:35.236 : 
epoch:6/10
--------------
train loss: 1.442e-02, train edgeJaccard: 0.412
val loss: 9.918e-03, val edgeJaccard: 0.496
--------------
23-08-02 14:36:35.237 : Trial number 16 pruned. Wasted 0hs 39min 15s on training ¯\_(ツ)_/¯
23-08-02 14:36:35.243 : Trial number 17 with parameters:
lr = 0.04271878748117173
tv_weight = 4.130530591516034e-06
23-08-02 14:43:16.988 : 
epoch:1/10
--------------
train loss: 2.035e-02, train edgeJaccard: 0.408
val loss: 1.187e-02, val edgeJaccard: 0.482
--------------
23-08-02 14:49:53.633 : 
epoch:2/10
--------------
train loss: 1.924e-02, train edgeJaccard: 0.386
val loss: 1.114e-02, val edgeJaccard: 0.477
--------------
23-08-02 14:56:26.457 : 
epoch:3/10
--------------
train loss: 1.748e-02, train edgeJaccard: 0.416
val loss: 1.133e-02, val edgeJaccard: 0.468
--------------
23-08-02 15:03:01.752 : 
epoch:4/10
--------------
train loss: 1.769e-02, train edgeJaccard: 0.397
val loss: 1.162e-02, val edgeJaccard: 0.475
--------------
23-08-02 15:09:37.854 : 
epoch:5/10
--------------
train loss: 1.614e-02, train edgeJaccard: 0.403
val loss: 9.913e-03, val edgeJaccard: 0.488
--------------
23-08-02 15:16:14.138 : 
epoch:6/10
--------------
train loss: 1.431e-02, train edgeJaccard: 0.420
val loss: 9.410e-03, val edgeJaccard: 0.498
--------------
23-08-02 15:22:49.267 : 
epoch:7/10
--------------
train loss: 1.426e-02, train edgeJaccard: 0.424
val loss: 1.023e-02, val edgeJaccard: 0.497
--------------
23-08-02 15:29:24.169 : 
epoch:8/10
--------------
train loss: 1.405e-02, train edgeJaccard: 0.452
val loss: 9.578e-03, val edgeJaccard: 0.498
--------------
23-08-02 15:35:57.431 : 
epoch:9/10
--------------
train loss: 1.334e-02, train edgeJaccard: 0.423
val loss: 9.142e-03, val edgeJaccard: 0.501
--------------
23-08-02 15:42:30.877 : 
epoch:10/10
--------------
train loss: 1.319e-02, train edgeJaccard: 0.443
val loss: 9.354e-03, val edgeJaccard: 0.503
--------------
23-08-02 15:42:30.877 : Trial 17: training completed in 1hs 65min 55s
23-08-02 15:42:30.884 : Trial number 18 with parameters:
lr = 0.04836178019925654
tv_weight = 3.65065447408984e-06
23-08-02 15:49:05.125 : 
epoch:1/10
--------------
train loss: 2.106e-02, train edgeJaccard: 0.419
val loss: 1.250e-02, val edgeJaccard: 0.480
--------------
23-08-02 15:55:38.490 : 
epoch:2/10
--------------
train loss: 1.898e-02, train edgeJaccard: 0.415
val loss: 1.147e-02, val edgeJaccard: 0.477
--------------
23-08-02 16:02:13.599 : 
epoch:3/10
--------------
train loss: 1.834e-02, train edgeJaccard: 0.419
val loss: 1.140e-02, val edgeJaccard: 0.468
--------------
23-08-02 16:08:51.966 : 
epoch:4/10
--------------
train loss: 1.789e-02, train edgeJaccard: 0.428
val loss: 1.191e-02, val edgeJaccard: 0.480
--------------
23-08-02 16:15:25.509 : 
epoch:5/10
--------------
train loss: 1.618e-02, train edgeJaccard: 0.407
val loss: 1.038e-02, val edgeJaccard: 0.495
--------------
23-08-02 16:21:57.650 : 
epoch:6/10
--------------
train loss: 1.435e-02, train edgeJaccard: 0.425
val loss: 1.010e-02, val edgeJaccard: 0.501
--------------
23-08-02 16:28:29.372 : 
epoch:7/10
--------------
train loss: 1.393e-02, train edgeJaccard: 0.430
val loss: 9.875e-03, val edgeJaccard: 0.501
--------------
23-08-02 16:35:02.576 : 
epoch:8/10
--------------
train loss: 1.344e-02, train edgeJaccard: 0.400
val loss: 9.679e-03, val edgeJaccard: 0.499
--------------
23-08-02 16:41:35.033 : 
epoch:9/10
--------------
train loss: 1.329e-02, train edgeJaccard: 0.417
val loss: 9.824e-03, val edgeJaccard: 0.501
--------------
23-08-02 16:48:07.118 : 
epoch:10/10
--------------
train loss: 1.306e-02, train edgeJaccard: 0.415
val loss: 9.757e-03, val edgeJaccard: 0.504
--------------
23-08-02 16:48:07.119 : Trial 18: training completed in 1hs 65min 36s
23-08-02 16:48:07.126 : Trial number 19 with parameters:
lr = 0.005393051054141455
tv_weight = 5.505566334861409e-07
23-08-02 16:54:40.907 : 
epoch:1/10
--------------
train loss: 2.057e-02, train edgeJaccard: 0.402
val loss: 1.162e-02, val edgeJaccard: 0.480
--------------
23-08-02 17:01:25.880 : 
epoch:2/10
--------------
train loss: 1.922e-02, train edgeJaccard: 0.353
val loss: 1.238e-02, val edgeJaccard: 0.475
--------------
23-08-02 17:08:00.796 : 
epoch:3/10
--------------
train loss: 1.842e-02, train edgeJaccard: 0.387
val loss: 1.101e-02, val edgeJaccard: 0.473
--------------
23-08-02 17:14:47.807 : 
epoch:4/10
--------------
train loss: 1.745e-02, train edgeJaccard: 0.412
val loss: 1.247e-02, val edgeJaccard: 0.475
--------------
23-08-02 17:21:26.419 : 
epoch:5/10
--------------
train loss: 1.696e-02, train edgeJaccard: 0.421
val loss: 9.553e-03, val edgeJaccard: 0.491
--------------
23-08-02 17:28:15.156 : 
epoch:6/10
--------------
train loss: 1.455e-02, train edgeJaccard: 0.404
val loss: 9.699e-03, val edgeJaccard: 0.496
--------------
23-08-02 17:28:15.157 : Trial number 19 pruned. Wasted 0hs 40min 7s on training ¯\_(ツ)_/¯
23-08-02 17:28:15.158 : Best trial:
FrozenTrial(number=18, state=TrialState.COMPLETE, values=[0.5042917094395694], datetime_start=datetime.datetime(2023, 8, 2, 15, 42, 30, 878524), datetime_complete=datetime.datetime(2023, 8, 2, 16, 48, 7, 120098), params={'lr': 0.04836178019925654, 'tv_weight': 3.65065447408984e-06}, user_attrs={}, system_attrs={}, intermediate_values={0: 0.4795577451580878, 1: 0.4795577451580878, 2: 0.4795577451580878, 3: 0.48040343820830905, 4: 0.4949801756465354, 5: 0.5012940234109039, 6: 0.5012940234109039, 7: 0.5012940234109039, 8: 0.5012940234109039, 9: 0.5042917094395694}, distributions={'lr': FloatDistribution(high=0.1, log=True, low=1e-05, step=None), 'tv_weight': FloatDistribution(high=0.01, log=True, low=1e-07, step=None)}, trial_id=18, value=None)
23-08-02 17:28:15.158 : Saving study information at optuna_hparams
23-08-02 17:28:15.582 : Hyperparameters study ended
