{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si CUDA está disponible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo y moverlo a la GPU\n",
    "class ComplexModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 64) # Capa de entrada con 2 neuronas y 64 neuronas de salida\n",
    "        self.fc2 = nn.Linear(64, 128) # Capa oculta con 64 neuronas de entrada y 128 neuronas de salida\n",
    "        self.fc3 = nn.Linear(128, 64) # Capa oculta con 128 neuronas de entrada y 64 neuronas de salida\n",
    "        self.fc4 = nn.Linear(64, 1) # Capa de salida con 64 neuronas de entrada y 1 neurona de salida\n",
    "        self.relu = nn.ReLU() # Función de activación ReLU\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplexModel(\n",
       "  (fc1): Linear(in_features=2, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ComplexModel()\n",
    "model.to(device) # Mover el modelo a la GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar datos y etiquetas\n",
    "numbers = np.arange(256)\n",
    "integers = np.arange(-4, 5)\n",
    "pairs = np.array(np.meshgrid(numbers, integers)).T.reshape(-1, 2)\n",
    "\n",
    "def get_num_ones(number):\n",
    "    binary = format(number, '08b')\n",
    "    num_ones = sum([int(bit) for bit in binary])\n",
    "    return num_ones\n",
    "\n",
    "labels = []\n",
    "for pair in pairs:\n",
    "    number = pair[0]\n",
    "    integer = pair[1]\n",
    "    num_ones = get_num_ones(number)\n",
    "    if (integer == 0) or num_ones == 4 - num_ones:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "\n",
    "pairs = torch.tensor(pairs, dtype=torch.float32).to(device)\n",
    "labels = torch.tensor(labels, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear función de pérdida y optimizador\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/2000000], Loss: 0.2457\n",
      "Epoch [2000/2000000], Loss: 0.2472\n",
      "Epoch [3000/2000000], Loss: 0.2399\n",
      "Epoch [4000/2000000], Loss: 0.2369\n",
      "Epoch [5000/2000000], Loss: 0.2361\n",
      "Epoch [6000/2000000], Loss: 0.2340\n",
      "Epoch [7000/2000000], Loss: 0.2324\n",
      "Epoch [8000/2000000], Loss: 0.2304\n",
      "Epoch [9000/2000000], Loss: 0.2506\n",
      "Epoch [10000/2000000], Loss: 0.2315\n",
      "Epoch [11000/2000000], Loss: 0.2295\n",
      "Epoch [12000/2000000], Loss: 0.2280\n",
      "Epoch [13000/2000000], Loss: 0.2597\n",
      "Epoch [14000/2000000], Loss: 0.2265\n",
      "Epoch [15000/2000000], Loss: 0.2252\n",
      "Epoch [16000/2000000], Loss: 0.2290\n",
      "Epoch [17000/2000000], Loss: 0.2258\n",
      "Epoch [18000/2000000], Loss: 0.2249\n",
      "Epoch [19000/2000000], Loss: 0.2241\n",
      "Epoch [20000/2000000], Loss: 0.2228\n",
      "Epoch [21000/2000000], Loss: 0.2252\n",
      "Epoch [22000/2000000], Loss: 0.2224\n",
      "Epoch [23000/2000000], Loss: 0.2210\n",
      "Epoch [24000/2000000], Loss: 0.2201\n",
      "Epoch [25000/2000000], Loss: 0.2190\n",
      "Epoch [26000/2000000], Loss: 0.2180\n",
      "Epoch [27000/2000000], Loss: 0.2166\n",
      "Epoch [28000/2000000], Loss: 0.2473\n",
      "Epoch [29000/2000000], Loss: 0.2212\n",
      "Epoch [30000/2000000], Loss: 0.2164\n",
      "Epoch [31000/2000000], Loss: 0.2155\n",
      "Epoch [32000/2000000], Loss: 0.2140\n",
      "Epoch [33000/2000000], Loss: 0.2242\n",
      "Epoch [34000/2000000], Loss: 0.2148\n",
      "Epoch [35000/2000000], Loss: 0.2125\n",
      "Epoch [36000/2000000], Loss: 0.2099\n",
      "Epoch [37000/2000000], Loss: 0.2083\n",
      "Epoch [38000/2000000], Loss: 0.2520\n",
      "Epoch [39000/2000000], Loss: 0.2136\n",
      "Epoch [40000/2000000], Loss: 0.2129\n",
      "Epoch [41000/2000000], Loss: 0.2122\n",
      "Epoch [42000/2000000], Loss: 0.2138\n",
      "Epoch [43000/2000000], Loss: 0.2120\n",
      "Epoch [44000/2000000], Loss: 0.2113\n",
      "Epoch [45000/2000000], Loss: 0.2207\n",
      "Epoch [46000/2000000], Loss: 0.2145\n",
      "Epoch [47000/2000000], Loss: 0.2122\n",
      "Epoch [48000/2000000], Loss: 0.2106\n",
      "Epoch [49000/2000000], Loss: 0.2100\n",
      "Epoch [50000/2000000], Loss: 0.2092\n",
      "Epoch [51000/2000000], Loss: 0.2081\n",
      "Epoch [52000/2000000], Loss: 0.2075\n",
      "Epoch [53000/2000000], Loss: 0.2074\n",
      "Epoch [54000/2000000], Loss: 0.2062\n",
      "Epoch [55000/2000000], Loss: 0.2056\n",
      "Epoch [56000/2000000], Loss: 0.2052\n",
      "Epoch [57000/2000000], Loss: 0.2057\n",
      "Epoch [58000/2000000], Loss: 0.2046\n",
      "Epoch [59000/2000000], Loss: 0.2043\n",
      "Epoch [60000/2000000], Loss: 0.2045\n",
      "Epoch [61000/2000000], Loss: 0.2044\n",
      "Epoch [62000/2000000], Loss: 0.2082\n",
      "Epoch [63000/2000000], Loss: 0.2071\n",
      "Epoch [64000/2000000], Loss: 0.2067\n",
      "Epoch [65000/2000000], Loss: 0.2064\n",
      "Epoch [66000/2000000], Loss: 0.2072\n",
      "Epoch [67000/2000000], Loss: 0.2062\n",
      "Epoch [68000/2000000], Loss: 0.2058\n",
      "Epoch [69000/2000000], Loss: 0.2060\n",
      "Epoch [70000/2000000], Loss: 0.2058\n",
      "Epoch [71000/2000000], Loss: 0.2053\n",
      "Epoch [72000/2000000], Loss: 0.2043\n",
      "Epoch [73000/2000000], Loss: 0.2040\n",
      "Epoch [74000/2000000], Loss: 0.2050\n",
      "Epoch [75000/2000000], Loss: 0.2040\n",
      "Epoch [76000/2000000], Loss: 0.2033\n",
      "Epoch [77000/2000000], Loss: 0.2058\n",
      "Epoch [78000/2000000], Loss: 0.2036\n",
      "Epoch [79000/2000000], Loss: 0.2033\n",
      "Epoch [80000/2000000], Loss: 0.2033\n",
      "Epoch [81000/2000000], Loss: 0.2029\n",
      "Epoch [82000/2000000], Loss: 0.2028\n",
      "Epoch [83000/2000000], Loss: 0.2038\n",
      "Epoch [84000/2000000], Loss: 0.2034\n",
      "Epoch [85000/2000000], Loss: 0.2030\n",
      "Epoch [86000/2000000], Loss: 0.2034\n",
      "Epoch [87000/2000000], Loss: 0.2029\n",
      "Epoch [88000/2000000], Loss: 0.2027\n",
      "Epoch [89000/2000000], Loss: 0.2021\n",
      "Epoch [90000/2000000], Loss: 0.2018\n",
      "Epoch [91000/2000000], Loss: 0.2020\n",
      "Epoch [92000/2000000], Loss: 0.2014\n",
      "Epoch [93000/2000000], Loss: 0.2009\n",
      "Epoch [94000/2000000], Loss: 0.2011\n",
      "Epoch [95000/2000000], Loss: 0.2001\n",
      "Epoch [96000/2000000], Loss: 0.1996\n",
      "Epoch [97000/2000000], Loss: 0.1997\n",
      "Epoch [98000/2000000], Loss: 0.1996\n",
      "Epoch [99000/2000000], Loss: 0.1995\n",
      "Epoch [100000/2000000], Loss: 0.1990\n",
      "Epoch [101000/2000000], Loss: 0.1995\n",
      "Epoch [102000/2000000], Loss: 0.1997\n",
      "Epoch [103000/2000000], Loss: 0.1995\n",
      "Epoch [104000/2000000], Loss: 0.1993\n",
      "Epoch [105000/2000000], Loss: 0.2261\n",
      "Epoch [106000/2000000], Loss: 0.1985\n",
      "Epoch [107000/2000000], Loss: 0.1988\n",
      "Epoch [108000/2000000], Loss: 0.1984\n",
      "Epoch [109000/2000000], Loss: 0.1982\n",
      "Epoch [110000/2000000], Loss: 0.1979\n",
      "Epoch [111000/2000000], Loss: 0.1982\n",
      "Epoch [112000/2000000], Loss: 0.1985\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m outputs \u001b[39m=\u001b[39m model(pairs)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Calcular la pérdida\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     10\u001b[0m \u001b[39m# Reiniciar gradientes y realizar retropropagación\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 720\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target,\n\u001b[1;32m    721\u001b[0m                                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    722\u001b[0m                                               pos_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_weight,\n\u001b[1;32m    723\u001b[0m                                               reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:3162\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (target\u001b[39m.\u001b[39msize() \u001b[39m==\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()):\n\u001b[1;32m   3160\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTarget size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) must be the same as input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize()))\n\u001b[0;32m-> 3162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbinary_cross_entropy_with_logits(\u001b[39minput\u001b[39;49m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "epochs = 100000000\n",
    "for epoch in range(epochs):\n",
    "    # Pasar datos de entrada a través del modelo y obtener predicciones\n",
    "    outputs = model(pairs)\n",
    "    \n",
    "    # Calcular la pérdida\n",
    "    loss = criterion(outputs, labels.unsqueeze(1))\n",
    "    \n",
    "    # Reiniciar gradientes y realizar retropropagación\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Actualizar los pesos del modelo\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Imprimir la pérdida en cada epoch\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.833333333333336\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "with torch.no_grad():\n",
    "    # Pasar datos de entrada a través del modelo y obtener predicciones\n",
    "    outputs = model(pairs)\n",
    "    \n",
    "    # Redondear las predicciones a 0 o 1\n",
    "    predictions = torch.round(torch.sigmoid(outputs))\n",
    "    \n",
    "    # Calcular la precisión\n",
    "    accuracy = (predictions == labels.unsqueeze(1)).sum().item() / labels.size(0) * 100\n",
    "    print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perdí la cuenta de cuántas veces le di play pero deben ser como 15 millones de épocas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
