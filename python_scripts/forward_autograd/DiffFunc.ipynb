{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from scipy import signal\n",
    "#import time #testing de tiempos\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "#import cProfile #testing de tiempos\n",
    "from joblib import Parallel, delayed  #multithreading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una red neuronal simple\n",
    "class Model_qm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_qm, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_m_XOR = Model_qm()\n",
    "state_dict_XOR = torch.load('q_m_XOR.pth')\n",
    "q_m_XOR.load_state_dict(state_dict_XOR)\n",
    "#input must be tensor with float (each bit)\n",
    "def q_m_XOR_diff(bits):\n",
    "    return q_m_XOR(bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_m_XNOR = Model_qm()\n",
    "state_dict_XNOR = torch.load('q_m_XNOR.pth')\n",
    "q_m_XNOR.load_state_dict(state_dict_XNOR)\n",
    "#input must be tensor with float (each bit)\n",
    "def q_m_XNOR_diff(bits):\n",
    "    return q_m_XNOR(bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit)\n",
    "def q_m_diff(input):\n",
    "    output = torch.tensor([input[0], 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.float32)\n",
    "    num_1 = torch.sum(input > 0.5)\n",
    "    if (num_1 > 4) or (num_1 == 4 and input[0] == 0):\n",
    "        output[:8] = q_m_XNOR_diff(input)\n",
    "        output[8] = 0\n",
    "    else:\n",
    "        output[:8] = q_m_XOR_diff(input)\n",
    "        output[8] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit) example: [0.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0], 2.0\n",
    "#output are tensors\n",
    "def TMDS_diff(pixel_bits,cnt):\n",
    "    bits_inversos = torch.flip(pixel_bits, dims = (0,))\n",
    "    #print(bits_inversos)\n",
    "    #bits_inversos = pixel_bits[::-1]\n",
    "\n",
    "    ###bits_inversos tiene grad_fn###\n",
    "    q_m = q_m_diff(bits_inversos)\n",
    "    ###q_m_diff tiene grad_fn###\n",
    "\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float32)\n",
    "    num_1 = torch.sum(q_m[:8] > 0.5)\n",
    "    num_0 = torch.sum(q_m[:8] < 0.5)\n",
    "    IndB = q_m[8] > 0.5\n",
    "    ###IndB_diff tiene grad_fn###\n",
    "\n",
    "    IndE = cnt == 0 or (num_1 == num_0)\n",
    "    ###IndE_diff tiene grad_fn###\n",
    "    \n",
    "    IndC = (cnt > 0 and (num_1 > num_0)) or (cnt < 0 and (num_0 > num_1))\n",
    "    ###IndC_diff tiene grad_fn###\n",
    "\n",
    "    Neg_q = 1 - q_m\n",
    "\n",
    "    IndD = q_m[8] < 0.5\n",
    "    ###InD_diff tiene grad_fn###\n",
    "    \n",
    "    if (IndE and IndB) or (not(IndC) and not(IndE)):\n",
    "        output[:8] = q_m[:8]\n",
    "    else:\n",
    "        output[:8] = Neg_q[:8]\n",
    "    output[8] = q_m[8]\n",
    "    if IndE:\n",
    "        output[9] = Neg_q[8]\n",
    "        if IndD:\n",
    "            new_cnt = cnt + num_0 - num_1\n",
    "        else:\n",
    "            new_cnt = cnt + num_1 - num_0\n",
    "    else:\n",
    "        if IndC:\n",
    "            output[9] = 1\n",
    "            new_cnt = cnt + 2 * q_m[8] + num_0 - num_1\n",
    "        else:\n",
    "            output[9] = 0\n",
    "            new_cnt = cnt - 2 * Neg_q[8] + num_1 - num_0\n",
    "    return output,new_cnt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMBIOS...\n",
    "\n",
    "EN FUNCION PIXEL2BIT_DIFF: CAMBIO DEFINICION DE OUTPUT (ANTES LISTA, AHORA TORCH.TENSOR)\n",
    "EN FUNCION TMDS_DIFF: CAMBIO MODO DE INVERSION DE ARRAY (USANDO TORCH.FLIP)\n",
    "EN FUNCION QM_DIFF: POR CAMBIOS EN OTRAS FUNCIONES, EL INPUT YA ES UN TORCH.TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x_in = x.clone()\n",
    "    if x >= 0:\n",
    "        return 1 / (1 + torch.exp(-x_in))\n",
    "    else:\n",
    "        return torch.exp(x_in) / (1 + torch.exp(x_in))\n",
    "\n",
    "def Pixel2Bit_diff(pixel):\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype= torch.float32)\n",
    "    for i in range(1,9):\n",
    "        output[i-1] = sigmoid(10*(pixel-2**(8-i)+0.5))  # 0.5 para ajustar la sigmoidal\n",
    "        if pixel >= 2**(8-i):\n",
    "            pixel = pixel - 2**(8-i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion que toma como entrada el armonico a sintonizar y las dimensiones de la imagen a espiar y devuelve un array con taps de g(t)\n",
    "def g_taps(dim_vertical, dim_horizontal, armonico):\n",
    "\n",
    "    #defino variables iniciales\n",
    "    f_b = 10 * (dim_vertical * dim_horizontal * 60)\n",
    "    f_sdr = 50e6\n",
    "    harm = armonico * f_b\n",
    "    \n",
    "    #para el correcto funcionamiento: dependiendo del armonico, elijo cuantas muestras por pulso\n",
    "    if (armonico < 5 ):\n",
    "        muestras_por_pulso  = 10\n",
    "    else:\n",
    "        muestras_por_pulso  = 20\n",
    "\n",
    "    samp_rate = muestras_por_pulso * f_b\n",
    "    H_samples = dim_horizontal * muestras_por_pulso\n",
    "\n",
    "    #creo el pulso\n",
    "    t_continuous = np.linspace(start = 0, stop = H_samples/samp_rate, num = H_samples, endpoint= False)\n",
    "    pulso = np.zeros(H_samples)\n",
    "    pulso[:muestras_por_pulso] = 1\n",
    "\n",
    "    #traslado el espectro del pulso el armonico correspondiente\n",
    "    frec_armonico = np.exp(-2j*np.pi*harm*t_continuous)\n",
    "    pulso_complejo = pulso*frec_armonico\n",
    "\n",
    "    #creo el lpf del sdr\n",
    "    b, a = signal.butter(6, f_sdr/2, fs=samp_rate, btype='lowpass', analog=False)\n",
    "\n",
    "    #filtro con lpf el pulso multiplicado por armonico. El resultado es g\n",
    "    g_t = signal.lfilter(b, a, pulso_complejo)\n",
    "    g_t = signal.decimate(g_t,q = muestras_por_pulso)\n",
    "\n",
    "    # si armonico crece, necesito mas taps\n",
    "    if (armonico < 5):\n",
    "        g_t = g_t[:170]\n",
    "    else:\n",
    "        g_t = g_t[:3000]\n",
    "\n",
    "    g_t_max = np.max(np.abs(g_t))\n",
    " \n",
    "    g_t = g_t / g_t_max\n",
    "\n",
    "    return torch.tensor(g_t,dtype = torch.complex64).reshape(1,1,len(g_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TMDS_2_row(multiplicador,cant_filas,img,columnas):\n",
    "    bits_cod_cant_filas = torch.zeros((1,1,10*columnas*cant_filas),dtype = torch.complex64)\n",
    "    for i in range(multiplicador*cant_filas, cant_filas*(multiplicador+1)):\n",
    "        cnt = 0\n",
    "        bits_cod_fila =  torch.zeros(10*columnas, dtype = torch.complex64)\n",
    "        for j in range(columnas):\n",
    "            pixel = img[i,j]\n",
    "            ###pixel tiene grad_fn ###\n",
    "\n",
    "            pixel_bits = Pixel2Bit_diff(pixel)\n",
    "            ###pixel_bits tiene grad_fn ###\n",
    "            \n",
    "            pixel_cod,cnt = TMDS_diff(pixel_bits, cnt)\n",
    "            ###TMDS_diff tiene grad_fn\n",
    "            #print(pixel_cod)  \n",
    "            \n",
    "            bits_cod_fila[j*10:(j+1)*10] = pixel_cod\n",
    "            ###bits_cod_fila tiene grad_fn\n",
    "            \n",
    "        bits_cod_cant_filas[0,0,(10*columnas)*(i-multiplicador*cant_filas):(10*columnas)*((i-multiplicador*cant_filas)+1)] = bits_cod_fila\n",
    "    return bits_cod_cant_filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_mayor_divisor(a, b): \n",
    "    while b != 0:\n",
    "        a, b = b, a % b\n",
    "    return a\n",
    "\n",
    "def Calc_filas_por_thread(filas, threads):  #calculo la max cantidad de filas para que todos calculen la misma cantidad y no sobren filas, para pasarle a cada hilo de forma de optimizar el uso de los hilos. Pasarle de a pocas filas es ineficiente\n",
    "    divisor = encontrar_mayor_divisor(filas,threads)\n",
    "    return filas // divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(img, armonico, num_threads):\n",
    "    filas, columnas = img.shape\n",
    "    g_t = g_taps(filas, columnas, armonico)\n",
    "    size_g_t = g_t.numel()    \n",
    "    padding = (size_g_t - 10)//2\n",
    "    filas_por_thread = Calc_filas_por_thread(filas,num_threads)\n",
    "\n",
    "    result = Parallel(n_jobs=num_threads)(delayed(TMDS_2_row)(multiplicador,filas_por_thread,img,columnas) for multiplicador in range(num_threads)) # da mejores resultados por poquito el multiprocessing (Pool)\n",
    "    ###result tiene grad_fn###\n",
    "    #return result[0]\n",
    "    if num_threads == 1:\n",
    "        bits_TMDS = result[0]\n",
    "    else:\n",
    "        bits_TMDS = result[0]\n",
    "        for i in range(1, len(result)):\n",
    "            bits_TMDS = torch.cat((bits_TMDS, result[i]), dim = 2)\n",
    "    ###bits_TMDS tiene grad_fn\n",
    "    #return bits_TMDS\n",
    "    img_salida = nn.functional.conv1d(bits_TMDS, g_t, stride = 10, padding=padding, bias = None)[0,0,:].reshape((filas,columnas))\n",
    "    ###img_salida tiene grad_fn###\n",
    "        \n",
    "    return img_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0458e-05+1.7852e-07j,  2.7771e-05-3.9061e-06j,\n",
      "          5.2324e-04-2.5223e-05j,  ...,\n",
      "          7.8755e-01+6.7438e-02j, -2.7092e-01+6.0032e-02j,\n",
      "         -2.8534e-01+5.5735e-02j],\n",
      "        [ 7.8082e-01+6.7681e-02j, -2.8534e-01+5.5735e-02j,\n",
      "          7.8082e-01+6.7681e-02j,  ...,\n",
      "          7.8755e-01+6.7438e-02j, -2.7092e-01+6.0032e-02j,\n",
      "         -2.8534e-01+5.5735e-02j],\n",
      "        [ 7.8082e-01+6.7681e-02j, -2.8534e-01+5.5735e-02j,\n",
      "          7.8082e-01+6.7681e-02j,  ...,\n",
      "          7.8755e-01+6.7438e-02j, -2.7092e-01+6.0032e-02j,\n",
      "         -2.8534e-01+5.5735e-02j],\n",
      "        ...,\n",
      "        [ 7.8082e-01+6.7681e-02j, -2.8534e-01+5.5735e-02j,\n",
      "          7.8082e-01+6.7681e-02j,  ...,\n",
      "          7.8755e-01+6.7438e-02j, -2.7092e-01+6.0032e-02j,\n",
      "         -2.8534e-01+5.5735e-02j],\n",
      "        [ 7.8082e-01+6.7681e-02j, -2.8534e-01+5.5735e-02j,\n",
      "          7.8082e-01+6.7681e-02j,  ...,\n",
      "          7.8755e-01+6.7438e-02j, -2.7092e-01+6.0032e-02j,\n",
      "         -2.8534e-01+5.5735e-02j],\n",
      "        [ 7.8082e-01+6.7681e-02j, -2.8534e-01+5.5735e-02j,\n",
      "          7.8082e-01+6.7681e-02j,  ...,\n",
      "          7.8755e-01+6.7438e-02j, -2.7092e-01+6.0032e-02j,\n",
      "         -2.8534e-01+5.5734e-02j]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = '../../images/VAMO!!.png'\n",
    "img = np.zeros((96,96))\n",
    "#img = np.asarray(Image.open(image_path))[:,:,0] #solo canal rojo\n",
    "img_torch = torch.tensor(img, dtype = torch.float32, requires_grad= True)\n",
    "\n",
    "armonico = 3\n",
    "num_threads = 1 # elegir numeros de hilos del cpu (dejar alguno libre para que no explote la PC)\n",
    "#cProfile.runctx('forward(imag,armonico)', globals(), locals()) \n",
    "imagen = forward(img_torch, armonico,num_threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagen.backward(gradient=torch.ones_like(imagen),retain_graph=True)\n",
    "#imagen.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(img_torch.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_sim = np.abs(imagen.detach().numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(255*((img_sim - img_sim.min())/(img_sim.max()-img_sim.min())),cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
