{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from scipy import signal\n",
    "#import time #testing de tiempos\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "#import cProfile #testing de tiempos\n",
    "from joblib import Parallel, delayed  #multithreading \n",
    "from torch.utils.checkpoint import checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegBit_diff(x):\n",
    "    x_in = x.clone()\n",
    "    return 1 / (1 + torch.exp(-(-15.43*x_in + 7.51)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Real2Bit_diff(x):\n",
    "    #print(x)\n",
    "    #x_in = torch.empty((1), dtype=torch.float64)\n",
    "    x_in = x.clone()\n",
    "    return 1 / (1 + torch.exp(-(-5.43 + 10.81*x_in)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XNOR_diff(x1, x2):\n",
    "    fc1_weight = torch.tensor([[4.7662,  4.8464],[-0.1215,  1.3168],[ 2.6322,  2.3068],[-0.0951, -0.4683]],dtype=torch.float32)\n",
    "    fc1_bias = torch.tensor([-1.5654,  0.3904, -3.6841, -0.0062],dtype=torch.float32)\n",
    "    fc2_weight = torch.tensor([[-5.6381,  1.5594,  5.0998, -0.2311]],dtype=torch.float32)\n",
    "    fc2_bias = torch.tensor([1.7433],dtype=torch.float32)\n",
    "\n",
    "    #x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    x = torch.empty((2),dtype=torch.float32)\n",
    "    x[0] = x1\n",
    "    x[1] = x2\n",
    "    \n",
    "    fc1_output = torch.sigmoid(torch.matmul(x, fc1_weight.transpose(0, 1)) + fc1_bias)\n",
    "    fc2_output = torch.sigmoid(torch.matmul(fc1_output, fc2_weight.transpose(0, 1)) + fc2_bias)\n",
    "    #print(fc2_output)\n",
    "    return fc2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_diff(x1, x2):\n",
    "    fc1_weight = torch.tensor([[-3.8973,  4.0330],[-4.1031,  3.8928],[-0.6775,  0.8108],[ 2.3834, -2.4550]],dtype=torch.float32)\n",
    "    fc1_bias = torch.tensor([ 2.2917, -2.1209, -0.6751, -1.6954],dtype=torch.float32)\n",
    "    fc2_weight = torch.tensor([[-4.5110,  5.7411,  1.0059,  2.4664]],dtype=torch.float32)\n",
    "    fc2_bias = torch.tensor([0.8839],dtype=torch.float32)\n",
    "\n",
    "    #x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    x = torch.empty((2),dtype=torch.float32)\n",
    "    x[0] = x1\n",
    "    x[1] = x2\n",
    "    fc1_output = torch.sigmoid(torch.matmul(x, fc1_weight.transpose(0, 1)) + fc1_bias)\n",
    "    fc2_output = torch.sigmoid(torch.matmul(fc1_output, fc2_weight.transpose(0, 1)) + fc2_bias)\n",
    "\n",
    "    return fc2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_diff(x1, x2):\n",
    "    weight = torch.tensor([[13.5223, 13.5223]],dtype=torch.float32)\n",
    "    bias = torch.tensor([-6.3024],dtype=torch.float32)\n",
    "    #x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    x = torch.empty((2),dtype=torch.float32)\n",
    "    x[0] = x1\n",
    "    x[1] = x2\n",
    "    output = torch.sigmoid(torch.matmul(x, weight.transpose(0, 1)) + bias)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit)\n",
    "def q_m_diff(input):\n",
    "    output = torch.tensor([input[0], 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.float32)\n",
    "    num_1 = torch.sum(input > 0.5)\n",
    "    if (num_1 > 4) or (num_1 == 4 and input[0] == 0):\n",
    "        for bit in range(1, 8):\n",
    "            output[bit] = Real2Bit_diff(XNOR_diff(output[bit-1], input[bit]))\n",
    "        output[8] = 0\n",
    "    else:\n",
    "        for bit in range(1, 8):\n",
    "            output[bit] = Real2Bit_diff(XOR_diff(output[bit-1], input[bit]))\n",
    "        output[8] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit) example: [0.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0], 2.0\n",
    "#output are tensors\n",
    "def TMDS_diff(pixel_bits,cnt):\n",
    "    bits_inversos = torch.flip(pixel_bits, dims = (0,))\n",
    "    #print(bits_inversos)\n",
    "    #bits_inversos = pixel_bits[::-1]\n",
    "\n",
    "    ###bits_inversos tiene grad_fn###\n",
    "    q_m = q_m_diff(bits_inversos)\n",
    "    ###q_m_diff tiene grad_fn###\n",
    "\n",
    "    q_m = Real2Bit_diff(q_m)\n",
    "    ###Real2Bit_diff tiene grad_fn###\n",
    "\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float32)\n",
    "    num_1 = torch.sum(q_m[:8] > 0.5)\n",
    "    num_0 = torch.sum(q_m[:8] < 0.5)\n",
    "    IndB = q_m[8] > 0.5\n",
    "    ###IndB_diff tiene grad_fn###\n",
    "\n",
    "    IndE = cnt == 0 or (num_1 == num_0)\n",
    "    ###IndE_diff tiene grad_fn###\n",
    "    \n",
    "    IndC = (cnt > 0 and (num_1 > num_0)) or (cnt < 0 and (num_0 > num_1))\n",
    "    ###IndC_diff tiene grad_fn###\n",
    "\n",
    "    Neg_q = NegBit_diff(q_m[:9])\n",
    "    ###NegBit_diff tiene grad_fn###\n",
    "\n",
    "    IndD = q_m[8] < 0.5\n",
    "    ###InD_diff tiene grad_fn###\n",
    "    \n",
    "    if (IndE and IndB) or (not(IndC) and not(IndE)):\n",
    "        output[:8] = q_m[:8]\n",
    "    else:\n",
    "        output[:8] = Neg_q[:8]\n",
    "    output[8] = q_m[8]\n",
    "    if IndE:\n",
    "        output[9] = Neg_q[8]\n",
    "        if IndD:\n",
    "            new_cnt = cnt + num_0 - num_1\n",
    "        else:\n",
    "            new_cnt = cnt + num_1 - num_0\n",
    "    else:\n",
    "        if IndC:\n",
    "            output[9] = 1\n",
    "            new_cnt = cnt + 2 * q_m[8] + num_0 - num_1\n",
    "        else:\n",
    "            output[9] = 0\n",
    "            new_cnt = cnt - 2 * Neg_q[8] + num_1 - num_0\n",
    "    return output,new_cnt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAMBIOS...\n",
    "\n",
    "EN FUNCION PIXEL2BIT_DIFF: CAMBIO DEFINICION DE OUTPUT (ANTES LISTA, AHORA TORCH.TENSOR)\n",
    "EN FUNCION TMDS_DIFF: CAMBIO MODO DE INVERSION DE ARRAY (USANDO TORCH.FLIP)\n",
    "EN FUNCION QM_DIFF: POR CAMBIOS EN OTRAS FUNCIONES, EL INPUT YA ES UN TORCH.TENSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x_in = x.clone()\n",
    "    if x >= 0:\n",
    "        return 1 / (1 + torch.exp(-x_in))\n",
    "    else:\n",
    "        return torch.exp(x_in) / (1 + torch.exp(x_in))\n",
    "\n",
    "def Pixel2Bit_diff(pixel):\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype= torch.float32)\n",
    "    for i in range(1,9):\n",
    "        output[i-1] = sigmoid(10*(pixel-2**(8-i)+0.5))  # 0.5 para ajustar la sigmoidal\n",
    "        if pixel >= 2**(8-i):\n",
    "            pixel = pixel - 2**(8-i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion que toma como entrada el armonico a sintonizar y las dimensiones de la imagen a espiar y devuelve un array con taps de g(t)\n",
    "def g_taps(dim_vertical, dim_horizontal, armonico):\n",
    "\n",
    "    #defino variables iniciales\n",
    "    f_b = 10 * (dim_vertical * dim_horizontal * 60)\n",
    "    f_sdr = 50e6\n",
    "    harm = armonico * f_b\n",
    "    \n",
    "    #para el correcto funcionamiento: dependiendo del armonico, elijo cuantas muestras por pulso\n",
    "    if (armonico < 5 ):\n",
    "        muestras_por_pulso  = 10\n",
    "    else:\n",
    "        muestras_por_pulso  = 20\n",
    "\n",
    "    samp_rate = muestras_por_pulso * f_b\n",
    "    H_samples = dim_horizontal * muestras_por_pulso\n",
    "\n",
    "    #creo el pulso\n",
    "    t_continuous = np.linspace(start = 0, stop = H_samples/samp_rate, num = H_samples, endpoint= False)\n",
    "    pulso = np.zeros(H_samples)\n",
    "    pulso[:muestras_por_pulso] = 1\n",
    "\n",
    "    #traslado el espectro del pulso el armonico correspondiente\n",
    "    frec_armonico = np.exp(-2j*np.pi*harm*t_continuous)\n",
    "    pulso_complejo = pulso*frec_armonico\n",
    "\n",
    "    #creo el lpf del sdr\n",
    "    b, a = signal.butter(6, f_sdr/2, fs=samp_rate, btype='lowpass', analog=False)\n",
    "\n",
    "    #filtro con lpf el pulso multiplicado por armonico. El resultado es g\n",
    "    g_t = signal.lfilter(b, a, pulso_complejo)\n",
    "    g_t = signal.decimate(g_t,q = muestras_por_pulso)\n",
    "\n",
    "    # si armonico crece, necesito mas taps\n",
    "    if (armonico < 5):\n",
    "        g_t = g_t[:170]\n",
    "    else:\n",
    "        g_t = g_t[:3000]\n",
    "\n",
    "    g_t_max = np.max(np.abs(g_t))\n",
    " \n",
    "    g_t = g_t / g_t_max\n",
    "\n",
    "    return torch.tensor(g_t,dtype = torch.complex64).reshape(1,1,len(g_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TMDS_2_row(multiplicador,cant_filas,img,columnas):\n",
    "    \n",
    "    bits_cod_cant_filas = torch.zeros((1,1,10*columnas*cant_filas),dtype = torch.complex64)\n",
    "    for i in range(multiplicador*cant_filas, cant_filas*(multiplicador+1)):\n",
    "        cnt = 0\n",
    "        bits_cod_fila =  torch.zeros(10*columnas, dtype = torch.complex64)\n",
    "        for j in range(columnas):\n",
    "            pixel = img[i,j]\n",
    "            ###pixel tiene grad_fn ###\n",
    "\n",
    "            pixel_bits = Pixel2Bit_diff(pixel)\n",
    "            ###pixel_bits tiene grad_fn ###\n",
    "            \n",
    "            pixel_cod,cnt = TMDS_diff(pixel_bits, cnt)\n",
    "            ###TMDS_diff tiene grad_fn\n",
    "            #print(pixel_cod)  \n",
    "            \n",
    "            bits_cod_fila[j*10:(j+1)*10] = pixel_cod\n",
    "            ###bits_cod_fila tiene grad_fn\n",
    "            \n",
    "        bits_cod_cant_filas[0,0,(10*columnas)*(i-multiplicador*cant_filas):(10*columnas)*((i-multiplicador*cant_filas)+1)] = bits_cod_fila\n",
    "    return bits_cod_cant_filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper_TMDS_2_row(multiplicador,cant_filas,img,columnas):\n",
    "    bits_cod_cant_filas = checkpoint(TMDS_2_row,multiplicador,cant_filas,img,columnas)\n",
    "    print(bits_cod_cant_filas)\n",
    "    return bits_cod_cant_filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_mayor_divisor(a, b): \n",
    "    while b != 0:\n",
    "        a, b = b, a % b\n",
    "    return a\n",
    "\n",
    "def Calc_filas_por_thread(filas, threads):  #calculo la max cantidad de filas para que todos calculen la misma cantidad y no sobren filas, para pasarle a cada hilo de forma de optimizar el uso de los hilos. Pasarle de a pocas filas es ineficiente\n",
    "    divisor = encontrar_mayor_divisor(filas,threads)\n",
    "    return filas // divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(img, armonico, num_threads):\n",
    "    filas, columnas = img.shape\n",
    "    g_t = g_taps(filas, columnas, armonico)\n",
    "    size_g_t = g_t.numel()    \n",
    "    padding = (size_g_t - 10)//2\n",
    "    filas_por_thread = Calc_filas_por_thread(filas,num_threads)\n",
    "\n",
    "    result = Parallel(n_jobs=num_threads)(delayed(wrapper_TMDS_2_row)(multiplicador,filas_por_thread,img,columnas) for multiplicador in range(num_threads)) # da mejores resultados por poquito el multiprocessing (Pool)\n",
    "    ###result tiene grad_fn###\n",
    "    #return result[0]\n",
    "    if num_threads == 1:\n",
    "        bits_TMDS = result[0]\n",
    "    else:\n",
    "        bits_TMDS = result[0]\n",
    "        for i in range(1, len(result)):\n",
    "            bits_TMDS = torch.cat((bits_TMDS, result[i]), dim = 2)\n",
    "    ###bits_TMDS tiene grad_fn\n",
    "    #return bits_TMDS\n",
    "    img_salida = nn.functional.conv1d(bits_TMDS, g_t, stride = 10, padding=padding, bias = None)[0,0,:].reshape((filas,columnas))\n",
    "    ###img_salida tiene grad_fn###\n",
    "        \n",
    "    return img_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0047+0.j, 0.0053+0.j, 0.0053+0.j,  ..., 0.0053+0.j, 0.9954+0.j, 0.0000+0.j]]],\n",
      "       grad_fn=<CheckpointFunctionBackward>)\n",
      "tensor([[-3.9006e-06+3.0444e-06j,  1.2992e-05-8.7430e-06j,\n",
      "          1.2182e-04-8.9033e-05j,  ...,\n",
      "          2.0766e+00-1.5319e+00j, -1.5854e+00+1.1581e+00j,\n",
      "          2.0529e+00-1.5148e+00j],\n",
      "        [-1.6708e+00+1.2225e+00j,  2.0719e+00-1.5157e+00j,\n",
      "          9.3651e-01-6.8899e-01j,  ...,\n",
      "         -6.2340e-01+4.7340e-01j, -6.2340e-01+4.7340e-01j,\n",
      "         -6.2342e-01+4.7341e-01j],\n",
      "        [-6.2338e-01+4.7339e-01j, -6.2306e-01+4.7315e-01j,\n",
      "         -6.2220e-01+4.7249e-01j,  ...,\n",
      "         -7.3162e-01+5.1442e-01j,  9.5731e-01-6.7889e-01j,\n",
      "          1.5577e+00-1.1405e+00j],\n",
      "        ...,\n",
      "        [-1.4169e+00+1.0738e+00j,  2.2220e+00-1.5846e+00j,\n",
      "          2.7174e+00-2.0011e+00j,  ...,\n",
      "          2.1426e+00-1.5840e+00j, -1.2446e+00+9.0533e-01j,\n",
      "          7.7468e-01-5.7758e-01j],\n",
      "        [ 8.9060e-01-6.4377e-01j,  9.9413e-01-7.6383e-01j,\n",
      "          6.6360e-01-4.7606e-01j,  ...,\n",
      "          1.6786e+00-1.2325e+00j, -5.5752e-02+5.1233e-02j,\n",
      "          2.3266e+00-1.7022e+00j],\n",
      "        [-7.6895e-01+5.4872e-01j,  8.7765e-01-7.0521e-01j,\n",
      "         -9.3151e-01+6.7507e-01j,  ...,\n",
      "         -2.4149e-01+1.3903e-01j, -5.5395e-01+3.8837e-01j,\n",
      "          1.1490e+00-8.5091e-01j]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "image_path = '../../images/VAMO!!.png'\n",
    "#img = np.zeros((96,96))\n",
    "img = np.asarray(Image.open(image_path))[:,:,0] #solo canal rojo\n",
    "img_torch = torch.tensor(img, dtype = torch.float32, requires_grad= True)\n",
    "\n",
    "armonico = 3\n",
    "num_threads = 1 # elegir numeros de hilos del cpu (dejar alguno libre para que no explote la PC)\n",
    "#cProfile.runctx('forward(imag,armonico)', globals(), locals()) \n",
    "imagen = forward(img_torch, armonico,num_threads)\n",
    "print(imagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "imagen.backward(gradient=torch.ones_like(imagen),retain_graph=True)\n",
    "#imagen.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(img_torch.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_sim = np.abs(imagen.detach().numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(255*((img_sim - img_sim.min())/(img_sim.max()-img_sim.min())),cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
