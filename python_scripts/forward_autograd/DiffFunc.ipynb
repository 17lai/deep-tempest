{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "#from joblib import Parallel, delayed  #multithreading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una red neuronal simple\n",
    "class Model_qm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_qm, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_m_XOR = Model_qm()\n",
    "state_dict_XOR = torch.load('q_m_XOR.pth')\n",
    "q_m_XOR.load_state_dict(state_dict_XOR)\n",
    "#input must be tensor with float (each bit)\n",
    "def q_m_XOR_diff(bits):\n",
    "    return q_m_XOR(bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_m_XNOR = Model_qm()\n",
    "state_dict_XNOR = torch.load('q_m_XNOR.pth')\n",
    "q_m_XNOR.load_state_dict(state_dict_XNOR)\n",
    "#input must be tensor with float (each bit)\n",
    "def q_m_XNOR_diff(bits):\n",
    "    return q_m_XNOR(bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit)\n",
    "def q_m_diff(input):\n",
    "    output = torch.tensor([input[0], 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.float32)\n",
    "    num_1 = torch.sum(input > 0.5)\n",
    "    if (num_1 > 4) or (num_1 == 4 and input[0] == 0):\n",
    "        output[:8] = q_m_XNOR_diff(input)\n",
    "        output[8] = 0\n",
    "    else:\n",
    "        output[:8] = q_m_XOR_diff(input)\n",
    "        output[8] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit) example: [0.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0], 2.0\n",
    "#output are tensors\n",
    "def TMDS_diff(pixel_bits,cnt):\n",
    "    bits_inversos = torch.flip(pixel_bits, dims = (0,))\n",
    "    q_m = q_m_diff(bits_inversos)\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float32)\n",
    "    num_1 = torch.sum(q_m[:8] > 0.5)\n",
    "    num_0 = torch.sum(q_m[:8] < 0.5)\n",
    "    IndE = cnt == 0 or (num_1 == num_0)\n",
    "    IndC = (cnt > 0 and (num_1 > num_0)) or (cnt < 0 and (num_0 > num_1))\n",
    "    Neg_q = 1 - q_m\n",
    "\n",
    "    if (IndE and q_m[8] > 0.5) or (not(IndC) and not(IndE)):\n",
    "        output[:8] = q_m[:8]\n",
    "    else:\n",
    "        output[:8] = Neg_q[:8]\n",
    "    output[8] = q_m[8]\n",
    "    if IndE:\n",
    "        output[9] = Neg_q[8]\n",
    "        if q_m[8] < 0.5:\n",
    "            new_cnt = cnt + num_0 - num_1\n",
    "        else:\n",
    "            new_cnt = cnt + num_1 - num_0\n",
    "    else:\n",
    "        if IndC:\n",
    "            output[9] = 1\n",
    "            new_cnt = cnt + 2 * q_m[8] + num_0 - num_1\n",
    "        else:\n",
    "            output[9] = 0\n",
    "            new_cnt = cnt - 2 * Neg_q[8] + num_1 - num_0\n",
    "    return output,new_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x_in = x.clone()\n",
    "    if x >= 0:\n",
    "        return 1 / (1 + torch.exp(-x_in))\n",
    "    else:\n",
    "        return torch.exp(x_in) / (1 + torch.exp(x_in))\n",
    "\n",
    "def Pixel2Bit_diff(pixel):\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype= torch.float32)\n",
    "    for i in range(1,9):\n",
    "        output[i-1] = sigmoid(10*(pixel-2**(8-i)+0.5))  # 0.5 para ajustar la sigmoidal\n",
    "        if pixel >= 2**(8-i):\n",
    "            pixel = pixel - 2**(8-i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion que toma como entrada el armonico a sintonizar y las dimensiones de la imagen a espiar y devuelve un array con taps de g(t)\n",
    "def g_taps(dim_vertical, dim_horizontal, armonico):\n",
    "\n",
    "    #defino variables iniciales\n",
    "    f_b = 10 * (dim_vertical * dim_horizontal * 60)\n",
    "    f_sdr = 50e6\n",
    "    harm = armonico * f_b\n",
    "    \n",
    "    #para el correcto funcionamiento: dependiendo del armonico, elijo cuantas muestras por pulso\n",
    "    if (armonico < 5 ):\n",
    "        muestras_por_pulso  = 10\n",
    "    else:\n",
    "        muestras_por_pulso  = 20\n",
    "\n",
    "    samp_rate = muestras_por_pulso * f_b\n",
    "    H_samples = dim_horizontal * muestras_por_pulso\n",
    "\n",
    "    #creo el pulso\n",
    "    t_continuous = np.linspace(start = 0, stop = H_samples/samp_rate, num = H_samples, endpoint= False)\n",
    "    pulso = np.zeros(H_samples)\n",
    "    pulso[:muestras_por_pulso] = 1\n",
    "\n",
    "    #traslado el espectro del pulso el armonico correspondiente\n",
    "    frec_armonico = np.exp(-2j*np.pi*harm*t_continuous)\n",
    "    pulso_complejo = pulso*frec_armonico\n",
    "\n",
    "    #creo el lpf del sdr\n",
    "    b, a = signal.butter(6, f_sdr/2, fs=samp_rate, btype='lowpass', analog=False)\n",
    "\n",
    "    #filtro con lpf el pulso multiplicado por armonico. El resultado es g\n",
    "    g_t = signal.lfilter(b, a, pulso_complejo)\n",
    "    g_t = signal.decimate(g_t,q = muestras_por_pulso)\n",
    "\n",
    "    # si armonico crece, necesito mas taps\n",
    "    if (armonico < 5):\n",
    "        g_t = g_t[:100]\n",
    "    else:\n",
    "        g_t = g_t[:200]\n",
    "\n",
    "    g_t_max = np.max(np.abs(g_t))\n",
    " \n",
    "    g_t = g_t / g_t_max\n",
    "\n",
    "    return torch.tensor(g_t,dtype = torch.complex64).reshape(1,1,len(g_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TMDS_rows(img,columnas,g_t,padding):\n",
    "    cnt = 0\n",
    "    bits_cod_fila =  torch.zeros((1,1,10*columnas), dtype = torch.complex64)\n",
    "    for j in range(columnas):\n",
    "        pixel = img[j]\n",
    "        pixel_bits = Pixel2Bit_diff(pixel)\n",
    "        pixel_cod,cnt = TMDS_diff(pixel_bits, cnt)\n",
    "        bits_cod_fila[j*10:(j+1)*10] = pixel_cod\n",
    "    img_block = nn.functional.conv1d(bits_cod_fila, g_t, stride = 10, padding=padding, bias = None)[0,0,:].reshape(columnas)\n",
    "    img_block.backward(gradient=torch.ones_like(img_block),retain_graph = False)\n",
    "    grad_fila = img.grad\n",
    "    return img_block,grad_fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(img, armonico):\n",
    "    filas, columnas = img.shape\n",
    "    g_t = g_taps(filas, columnas, armonico)\n",
    "    size_g_t = g_t.numel()    \n",
    "    padding = (size_g_t - 10)//2\n",
    "    grads = []\n",
    "    for i in range(filas):\n",
    "        img_slice = torch.tensor(img[i,:], dtype=torch.float32, requires_grad=True)\n",
    "        img_fila,grad = TMDS_rows(img_slice,columnas,g_t,padding)\n",
    "        if i == 0:\n",
    "            img_salida = img_fila.detach()\n",
    "        else:\n",
    "            img_salida = torch.cat((img_salida, img_fila.detach()), dim = 0)\n",
    "        grads.append(grad.detach())\n",
    "        del grad\n",
    "        del img_fila\n",
    "        del img_slice\n",
    "    \n",
    "    return img_salida,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (2500) must match the existing size (10) at non-singleton dimension 2.  Target sizes: [1, 1, 2500].  Tensor sizes: [10]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m#img = np.asarray(Image.open(image_path))[:,:,0] #solo canal rojo\u001b[39;00m\n\u001b[1;32m      4\u001b[0m armonico \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m----> 5\u001b[0m imagen,grads \u001b[39m=\u001b[39m forward(img, armonico)\n",
      "Cell \u001b[0;32mIn[13], line 9\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(img, armonico)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(filas):\n\u001b[1;32m      8\u001b[0m     img_slice \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(img[i,:], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, requires_grad\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m     img_fila,grad \u001b[39m=\u001b[39m TMDS_rows(img_slice,columnas,g_t,padding)\n\u001b[1;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     11\u001b[0m         img_salida \u001b[39m=\u001b[39m img_fila\u001b[39m.\u001b[39mdetach()\n",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m, in \u001b[0;36mTMDS_rows\u001b[0;34m(img, columnas, g_t, padding)\u001b[0m\n\u001b[1;32m      6\u001b[0m     pixel_bits \u001b[39m=\u001b[39m Pixel2Bit_diff(pixel)\n\u001b[1;32m      7\u001b[0m     pixel_cod,cnt \u001b[39m=\u001b[39m TMDS_diff(pixel_bits, cnt)\n\u001b[0;32m----> 8\u001b[0m     bits_cod_fila[j\u001b[39m*\u001b[39;49m\u001b[39m10\u001b[39;49m:(j\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m*\u001b[39;49m\u001b[39m10\u001b[39;49m] \u001b[39m=\u001b[39m pixel_cod\n\u001b[1;32m      9\u001b[0m img_block \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mconv1d(bits_cod_fila, g_t, stride \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, padding\u001b[39m=\u001b[39mpadding, bias \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m)[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m,:]\u001b[39m.\u001b[39mreshape(columnas)\n\u001b[1;32m     10\u001b[0m img_block\u001b[39m.\u001b[39mbackward(gradient\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mones_like(img_block),retain_graph \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2500) must match the existing size (10) at non-singleton dimension 2.  Target sizes: [1, 1, 2500].  Tensor sizes: [10]"
     ]
    }
   ],
   "source": [
    "image_path = '../../images/VAMO!!.png'\n",
    "img = np.zeros((250,250))\n",
    "#img = np.asarray(Image.open(image_path))[:,:,0] #solo canal rojo\n",
    "armonico = 3\n",
    "imagen,grads = forward(img, armonico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_sim = np.abs(imagen.detach().numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(255*((img_sim - img_sim.min())/(img_sim.max()-img_sim.min())),cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
