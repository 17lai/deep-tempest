{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from scipy import signal\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "#from joblib import Parallel, delayed  #multithreading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una red neuronal simple\n",
    "class Model_qm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model_qm, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_m_XOR = Model_qm()\n",
    "state_dict_XOR = torch.load('q_m_XOR.pth')\n",
    "q_m_XOR.load_state_dict(state_dict_XOR)\n",
    "#input must be tensor with float (each bit)\n",
    "def q_m_XOR_diff(bits):\n",
    "    return q_m_XOR(bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_m_XNOR = Model_qm()\n",
    "state_dict_XNOR = torch.load('q_m_XNOR.pth')\n",
    "q_m_XNOR.load_state_dict(state_dict_XNOR)\n",
    "#input must be tensor with float (each bit)\n",
    "def q_m_XNOR_diff(bits):\n",
    "    return q_m_XNOR(bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit)\n",
    "def q_m_diff(input):\n",
    "    output = torch.tensor([input[0], 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.float32)\n",
    "    num_1 = torch.sum(input > 0.5)\n",
    "    if (num_1 > 4) or (num_1 == 4 and input[0] == 0):\n",
    "        output[:8] = q_m_XNOR_diff(input)\n",
    "        output[8] = 0\n",
    "    else:\n",
    "        output[:8] = q_m_XOR_diff(input)\n",
    "        output[8] = 1\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit) example: [0.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0], 2.0\n",
    "#output are tensors\n",
    "def TMDS_diff(pixel_bits,cnt):\n",
    "    bits_inversos = torch.flip(pixel_bits, dims = (0,))\n",
    "    q_m = q_m_diff(bits_inversos)\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float32)\n",
    "    num_1 = torch.sum(q_m[:8] > 0.5)\n",
    "    num_0 = torch.sum(q_m[:8] < 0.5)\n",
    "    IndE = cnt == 0 or (num_1 == num_0)\n",
    "    IndC = (cnt > 0 and (num_1 > num_0)) or (cnt < 0 and (num_0 > num_1))\n",
    "    Neg_q = 1 - q_m\n",
    "\n",
    "    if (IndE and q_m[8] > 0.5) or (not(IndC) and not(IndE)):\n",
    "        output[:8] = q_m[:8]\n",
    "    else:\n",
    "        output[:8] = Neg_q[:8]\n",
    "    output[8] = q_m[8]\n",
    "    if IndE:\n",
    "        output[9] = Neg_q[8]\n",
    "        if q_m[8] < 0.5:\n",
    "            new_cnt = cnt + num_0 - num_1\n",
    "        else:\n",
    "            new_cnt = cnt + num_1 - num_0\n",
    "    else:\n",
    "        if IndC:\n",
    "            output[9] = 1\n",
    "            new_cnt = cnt + 2 * q_m[8] + num_0 - num_1\n",
    "        else:\n",
    "            output[9] = 0\n",
    "            new_cnt = cnt - 2 * Neg_q[8] + num_1 - num_0\n",
    "    return output,new_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    x_in = x.clone()\n",
    "    if x >= 0:\n",
    "        return 1 / (1 + torch.exp(-x_in))\n",
    "    else:\n",
    "        return torch.exp(x_in) / (1 + torch.exp(x_in))\n",
    "\n",
    "def Pixel2Bit_diff(pixel):\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0], dtype= torch.float32)\n",
    "    for i in range(1,9):\n",
    "        output[i-1] = sigmoid(10*(pixel-2**(8-i)+0.5))  # 0.5 para ajustar la sigmoidal\n",
    "        if pixel >= 2**(8-i):\n",
    "            pixel = pixel - 2**(8-i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion que toma como entrada el armonico a sintonizar y las dimensiones de la imagen a espiar y devuelve un array con taps de g(t)\n",
    "def g_taps(dim_vertical, dim_horizontal, armonico):\n",
    "\n",
    "    #defino variables iniciales\n",
    "    f_b = 10 * (dim_vertical * dim_horizontal * 60)\n",
    "    f_sdr = 50e6\n",
    "    harm = armonico * f_b\n",
    "    \n",
    "    #para el correcto funcionamiento: dependiendo del armonico, elijo cuantas muestras por pulso\n",
    "    if (armonico < 5 ):\n",
    "        muestras_por_pulso  = 10\n",
    "    else:\n",
    "        muestras_por_pulso  = 20\n",
    "\n",
    "    samp_rate = muestras_por_pulso * f_b\n",
    "    H_samples = dim_horizontal * muestras_por_pulso\n",
    "\n",
    "    #creo el pulso\n",
    "    t_continuous = np.linspace(start = 0, stop = H_samples/samp_rate, num = H_samples, endpoint= False)\n",
    "    pulso = np.zeros(H_samples)\n",
    "    pulso[:muestras_por_pulso] = 1\n",
    "\n",
    "    #traslado el espectro del pulso el armonico correspondiente\n",
    "    frec_armonico = np.exp(-2j*np.pi*harm*t_continuous)\n",
    "    pulso_complejo = pulso*frec_armonico\n",
    "\n",
    "    #creo el lpf del sdr\n",
    "    b, a = signal.butter(6, f_sdr/2, fs=samp_rate, btype='lowpass', analog=False)\n",
    "\n",
    "    #filtro con lpf el pulso multiplicado por armonico. El resultado es g\n",
    "    g_t = signal.lfilter(b, a, pulso_complejo)\n",
    "    g_t = signal.decimate(g_t,q = muestras_por_pulso)\n",
    "\n",
    "    # si armonico crece, necesito mas taps\n",
    "    if (armonico < 5):\n",
    "        g_t = g_t[:170]\n",
    "    else:\n",
    "        g_t = g_t[:300]\n",
    "\n",
    "    g_t_max = np.max(np.abs(g_t))\n",
    " \n",
    "    g_t = g_t / g_t_max\n",
    "\n",
    "    return torch.tensor(g_t,dtype = torch.complex64).reshape(1,1,len(g_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TMDS_rows(img,columnas,g_t,padding, slice_num):\n",
    "    bits_cod_filas = torch.zeros((1,1,10*columnas*slice_num),dtype = torch.complex64)\n",
    "    for i in range(0, slice_num):\n",
    "        cnt = 0\n",
    "        bits_cod_fila =  torch.zeros(10*columnas, dtype = torch.complex64)\n",
    "        for j in range(columnas):\n",
    "            pixel = img[i,j]\n",
    "            pixel_bits = Pixel2Bit_diff(pixel)\n",
    "            pixel_cod,cnt = TMDS_diff(pixel_bits, cnt)\n",
    "            bits_cod_fila[j*10:(j+1)*10] = pixel_cod\n",
    "        bits_cod_filas[0,0,10*columnas*i:10*columnas*(i+1)] = bits_cod_fila\n",
    "    img_block = nn.functional.conv1d(bits_cod_filas, g_t, stride = 10, padding=padding, bias = None)[0,0,:].reshape((slice_num,columnas))\n",
    "    img_block.backward(gradient=torch.ones_like(img_block),retain_graph = False)\n",
    "    grad_cant_filas = img.grad\n",
    "    print(\"listo\")\n",
    "    return img_block,grad_cant_filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(img, armonico,num_grads):\n",
    "    filas, columnas = img.shape\n",
    "    g_t = g_taps(filas, columnas, armonico)\n",
    "    size_g_t = g_t.numel()    \n",
    "    padding = (size_g_t - 10)//2\n",
    "    slice_num = filas // num_grads  #necesito que de resto 0 para que ande, o sea divisor del numero de filas\n",
    "    grads = []\n",
    "    for j in range(num_grads):\n",
    "        img_slice = torch.tensor(img[j*slice_num:(j+1)*slice_num], dtype=torch.float32, requires_grad=True)\n",
    "        img_block,grad = TMDS_rows(img_slice,columnas,g_t,padding, slice_num)\n",
    "        if j == 0:\n",
    "            img_salida = img_block.detach()\n",
    "        else:\n",
    "            img_salida = torch.cat((img_salida, img_block.detach()), dim = 0)\n",
    "        grads.append(grad.detach())\n",
    "        del grad\n",
    "        del img_block\n",
    "        del img_slice\n",
    "    \n",
    "    return img_salida,grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listo\n",
      "listo\n"
     ]
    }
   ],
   "source": [
    "image_path = '../../images/VAMO!!.png'\n",
    "img = np.zeros((250,250))\n",
    "#img = np.asarray(Image.open(image_path))[:,:,0] #solo canal rojo\n",
    "armonico = 3\n",
    "num_grads = 2\n",
    "imagen = forward(img, armonico,num_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'detach'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img_sim \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(imagen\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m      3\u001b[0m plt\u001b[39m.\u001b[39mfigure()\n\u001b[1;32m      4\u001b[0m plt\u001b[39m.\u001b[39mimshow(\u001b[39m255\u001b[39m\u001b[39m*\u001b[39m((img_sim \u001b[39m-\u001b[39m img_sim\u001b[39m.\u001b[39mmin())\u001b[39m/\u001b[39m(img_sim\u001b[39m.\u001b[39mmax()\u001b[39m-\u001b[39mimg_sim\u001b[39m.\u001b[39mmin())),cmap\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'detach'"
     ]
    }
   ],
   "source": [
    "img_sim = np.abs(imagen.detach().numpy())\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(255*((img_sim - img_sim.min())/(img_sim.max()-img_sim.min())),cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
