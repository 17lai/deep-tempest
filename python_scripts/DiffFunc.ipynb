{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import signal\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be tensor with float (each bit)\n",
    "def IndA_diff(input):\n",
    "    weights = torch.tensor([[-7.8433e-04, 37.006, 37.006, 37.006, 37.005, 37.006, 37.006, 37.007]],dtype=torch.float64)\n",
    "    bias = torch.tensor([-131.3873],dtype=torch.float64)\n",
    "    linear_output = torch.matmul(input, weights.t()) + bias\n",
    "    output = torch.sigmoid(linear_output)\n",
    "\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo de regresión logística1\n",
    "class LogisticRegressionB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionB, self).__init__()\n",
    "        self.linear1 = nn.Linear(9, 20,dtype=torch.float64) \n",
    "        self.linear2 = nn.Linear(20, 10,dtype=torch.float64)\n",
    "        self.linear3 = nn.Linear(10, 1,dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo de regresión logística\n",
    "class LogisticRegressionD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionD, self).__init__()\n",
    "        self.linear1 = nn.Linear(9, 10,dtype=torch.float64) # 9 entradas y 20 salida\n",
    "        self.linear2 = nn.Linear(10, 5,dtype=torch.float64)\n",
    "        self.linear3 = nn.Linear(5, 1,dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndB = LogisticRegressionB()\n",
    "state_dictB = torch.load('indicatrizB.pth')\n",
    "IndB.load_state_dict(state_dictB)\n",
    "#input must be tensor with float (each bit)\n",
    "def IndB_diff(bits):\n",
    "    return IndB(bits).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 60,dtype=torch.float64)  \n",
    "        self.fc2 = nn.Linear(60, 30,dtype=torch.float64)\n",
    "        self.fc3 = nn.Linear(30, 20,dtype=torch.float64)\n",
    "        self.fc4 = nn.Linear(20, 1,dtype=torch.float64)    \n",
    "        self.sigmoid = nn.Sigmoid()  # Función de activación sigmoide\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndE = Net()\n",
    "state_dictE = torch.load('indicatrizE.pth')\n",
    "IndE.load_state_dict(state_dictE)\n",
    "#bits must be tensor and cnt integer\n",
    "def IndE_diff(bits,cnt):\n",
    "    cnt = torch.tensor([cnt],dtype=torch.float64)\n",
    "    input = torch.cat((bits,cnt))\n",
    "    return IndE(input).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndC = Net()\n",
    "state_dictC = torch.load('indicatrizC.pth')\n",
    "IndC.load_state_dict(state_dictC)\n",
    "#bits must be tensor and cnt integer\n",
    "def IndC_diff(bits,cnt):\n",
    "    cnt = torch.tensor([cnt],dtype=torch.float64)\n",
    "    input = torch.cat((bits,cnt))\n",
    "    return IndC(input).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndD = LogisticRegressionD()\n",
    "state_dictD = torch.load('indicatrizD.pth')\n",
    "IndD.load_state_dict(state_dictD)\n",
    "#input must be tensor with float (each bit)\n",
    "def IndD_diff(bits):\n",
    "    return IndD(bits).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegBit_diff(x):\n",
    "    return 1 / (1 + np.exp(-(-15.43*x + 7.51)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Real2Bit_diff(x):\n",
    "    return 1 / (1 + np.exp(-(-5.43 + 10.81*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XNOR_diff(x1, x2):\n",
    "    fc1_weight = torch.tensor([[4.7662,  4.8464],[-0.1215,  1.3168],[ 2.6322,  2.3068],[-0.0951, -0.4683]],dtype=torch.float64)\n",
    "    fc1_bias = torch.tensor([-1.5654,  0.3904, -3.6841, -0.0062],dtype=torch.float64)\n",
    "    fc2_weight = torch.tensor([[-5.6381,  1.5594,  5.0998, -0.2311]],dtype=torch.float64)\n",
    "    fc2_bias = torch.tensor([1.7433],dtype=torch.float64)\n",
    "\n",
    "    x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    fc1_output = torch.sigmoid(torch.matmul(x, fc1_weight.transpose(0, 1)) + fc1_bias)\n",
    "    fc2_output = torch.sigmoid(torch.matmul(fc1_output, fc2_weight.transpose(0, 1)) + fc2_bias)\n",
    "\n",
    "    return fc2_output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_diff(x1, x2):\n",
    "    fc1_weight = torch.tensor([[-3.8973,  4.0330],[-4.1031,  3.8928],[-0.6775,  0.8108],[ 2.3834, -2.4550]],dtype=torch.float64)\n",
    "    fc1_bias = torch.tensor([ 2.2917, -2.1209, -0.6751, -1.6954],dtype=torch.float64)\n",
    "    fc2_weight = torch.tensor([[-4.5110,  5.7411,  1.0059,  2.4664]],dtype=torch.float64)\n",
    "    fc2_bias = torch.tensor([0.8839],dtype=torch.float64)\n",
    "\n",
    "    x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    fc1_output = torch.sigmoid(torch.matmul(x, fc1_weight.transpose(0, 1)) + fc1_bias)\n",
    "    fc2_output = torch.sigmoid(torch.matmul(fc1_output, fc2_weight.transpose(0, 1)) + fc2_bias)\n",
    "\n",
    "    return fc2_output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_diff(x1, x2):\n",
    "    weight = torch.tensor([[13.5223, 13.5223]],dtype=torch.float64)\n",
    "    bias = torch.tensor([-6.3024],dtype=torch.float64)\n",
    "    x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    output = torch.sigmoid(torch.matmul(x, weight.transpose(0, 1)) + bias)\n",
    "\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit)\n",
    "def q_m_diff(input):\n",
    "    input_cp = input.copy()\n",
    "    t_input = torch.tensor(input_cp,dtype=torch.float64)\n",
    "    output = torch.tensor([t_input[0], 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float64)\n",
    "    for bit in range(1, 8):\n",
    "        output[bit] = IndA_diff(t_input) * Real2Bit_diff(XNOR_diff(output[bit-1], t_input[bit])) + (1 - IndA_diff(t_input)) * Real2Bit_diff(XOR_diff(output[bit-1],t_input[bit]))\n",
    "    output[8] = 1 - IndA_diff(t_input)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit) example: [0.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0], 2.0\n",
    "#output are tensors\n",
    "def TMDS_diff(pixel_bits,cnt):\n",
    "    bits_inversos = pixel_bits[::-1]\n",
    "    q_m = q_m_diff(bits_inversos)\n",
    "    q_m = Real2Bit_diff(q_m)\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float64)\n",
    "    for bit in range(0,8):\n",
    "        output[bit] = q_m[bit] * OR_diff(IndE_diff(q_m[:8],cnt) * IndB_diff(q_m[:9]),(1-IndC_diff(q_m[:8],cnt)) * (1-IndE_diff(q_m[:8],cnt))) + NegBit_diff(q_m[bit]) * OR_diff((1 - IndB_diff(q_m[:9])) * IndE_diff(q_m[:8],cnt),IndC_diff(q_m[:8],cnt) * (1 - IndE_diff(q_m[:8],cnt)))\n",
    "    output[8] = q_m[8]\n",
    "    output[9] = IndE_diff(q_m[:8],cnt) * NegBit_diff(q_m[8]) + (1 - IndE_diff(q_m[:8],cnt)) * IndC_diff(q_m[:8],cnt)\n",
    "    new_cnt = IndE_diff(q_m[:8],cnt) * ((cnt + torch.sum(q_m[:8] < 0.5).item() - torch.sum(q_m[:8] > 0.5).item()) * IndD_diff(q_m) + (cnt + torch.sum(q_m[:8] > 0.5).item() - torch.sum(q_m[:8] < 0.5).item()) * (1 - IndD_diff(q_m))) + (1 - IndE_diff(q_m[:8],cnt)) * ((cnt + 2 * q_m[8] + torch.sum(q_m[:8] < 0.5).item() - torch.sum(q_m[:8] > 0.5).item()) * IndC_diff(q_m[:8],cnt) + (cnt - 2 * NegBit_diff(q_m[8]) + torch.sum(q_m[:8] > 0.5).item() - torch.sum(q_m[:8] < 0.5).item()) * (1 - IndC_diff(q_m[:8],cnt)))\n",
    "    return output,new_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / (1 + np.exp(x))\n",
    "\n",
    "def Pixel2Bit_diff(pixel):\n",
    "    output = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(1,9):\n",
    "        output[i-1] = sigmoid(10*(pixel-2**(8-i)+0.5))  # 0.5 para ajustar la sigmoidal\n",
    "        if pixel >= 2**(8-i):\n",
    "            pixel = pixel - 2**(8-i)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion que toma como entrada el armonico a sintonizar y las dimensiones de la imagen a espiar y devuelve un array con taps de g(t)\n",
    "def g_taps(dim_vertical, dim_horizontal, armonico):\n",
    "\n",
    "    #defino variables iniciales\n",
    "    f_b = 10 * (dim_vertical * dim_horizontal * 60)\n",
    "    f_sdr = 50e6\n",
    "    harm = armonico * f_b\n",
    "    \n",
    "    #para el correcto funcionamiento: dependiendo del armonico, elijo cuantas muestras por pulso\n",
    "    if (armonico < 5 ):\n",
    "        muestras_por_pulso  = 10\n",
    "    else:\n",
    "        muestras_por_pulso  = 20\n",
    "\n",
    "    samp_rate = muestras_por_pulso * f_b\n",
    "    H_samples = dim_horizontal * muestras_por_pulso\n",
    "\n",
    "    #creo el pulso\n",
    "    t_continuous = np.linspace(start = 0, stop = H_samples/samp_rate, num = H_samples, endpoint= False)\n",
    "    pulso = np.zeros(H_samples)\n",
    "    pulso[:muestras_por_pulso] = 0.7/255\n",
    "\n",
    "    #traslado el espectro del pulso el armonico correspondiente\n",
    "    frec_armonico = np.exp(-2j*np.pi*harm*t_continuous)\n",
    "    pulso_complejo = pulso*frec_armonico\n",
    "\n",
    "    #creo el lpf del sdr\n",
    "    b, a = signal.butter(6, f_sdr/2, fs=samp_rate, btype='lowpass', analog=False)\n",
    "\n",
    "    #filtro con lpf el pulso multiplicado por armonico. El resultado es g\n",
    "    g_t = signal.lfilter(b, a, pulso_complejo)\n",
    "\n",
    "    # si arminico crece, necesito mas taps\n",
    "    if (armonico < 5):\n",
    "        g_t = g_t[:1500]\n",
    "    else:\n",
    "        g_t = g_t[:3000]\n",
    "\n",
    "    return torch.tensor(g_t,dtype = torch.complex64).reshape(1,1,len(g_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(img, armonico):\n",
    "    filas, columnas = img.shape\n",
    "    img_salida = img.copy()\n",
    "    bits_codificados_fila = torch.zeros((1,1,10*columnas),dtype = torch.complex64)\n",
    "    g_t = g_taps(filas, columnas, armonico)\n",
    "    size_g_t = g_t.numel()\n",
    "    for i in range(filas):\n",
    "        t1 = time.time()\n",
    "        cnt = 0\n",
    "        for j in range(columnas):\n",
    "            pixel = img[i,j]\n",
    "            pixel_bits = Pixel2Bit_diff(pixel)\n",
    "            bits_cods, cnt = TMDS_diff(pixel_bits, cnt)\n",
    "            bits_codificados_fila[0,0,j*10:(j+1)*10] = bits_cods\n",
    "        padding = (size_g_t - 8)//2\n",
    "        img_salida[i,:] = nn.functional.conv1d(bits_codificados_fila,g_t,stride = 10, padding = padding, bias = None)\n",
    "        t2=time.time()\n",
    "        print(t2-t1)\n",
    "        #convolucion = torch.nn.Conv1d(1, 1, kernel_size = size_g_t, stride = 10, padding = padding, bias = False, dtype = torch.complex64)\n",
    "        #convolucion.weight.data = g_t\n",
    "        #img_salida[i,:] = convolucion\n",
    "    return img_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.local/lib/python3.10/site-packages/torch/_tensor.py:958: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return self.numpy().astype(dtype, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8247034549713135\n",
      "1.7627880573272705\n",
      "1.7264704704284668\n",
      "1.816884994506836\n",
      "1.8544554710388184\n",
      "1.844186782836914\n",
      "1.8308422565460205\n",
      "1.7540721893310547\n",
      "1.7743566036224365\n",
      "1.8374011516571045\n",
      "1.8364574909210205\n",
      "1.8029303550720215\n",
      "1.7697064876556396\n",
      "1.8128385543823242\n",
      "1.82743239402771\n",
      "1.8423705101013184\n",
      "1.8142316341400146\n",
      "1.7926130294799805\n",
      "1.834010124206543\n",
      "1.8400721549987793\n",
      "1.8141965866088867\n",
      "1.7832679748535156\n",
      "1.8163044452667236\n",
      "1.800154209136963\n",
      "1.7046573162078857\n",
      "1.7268891334533691\n",
      "1.7128124237060547\n",
      "1.7160000801086426\n",
      "1.7307288646697998\n",
      "1.7686750888824463\n",
      "1.7598028182983398\n",
      "1.7527499198913574\n",
      "1.7506282329559326\n",
      "1.733384370803833\n",
      "1.7371504306793213\n",
      "1.8067638874053955\n",
      "1.8253209590911865\n",
      "1.798987627029419\n",
      "1.8070197105407715\n",
      "1.7844126224517822\n",
      "1.7302792072296143\n",
      "1.7146532535552979\n",
      "1.7088558673858643\n",
      "1.7403290271759033\n",
      "1.7113449573516846\n",
      "1.703138828277588\n",
      "1.7081942558288574\n",
      "1.720942735671997\n",
      "1.7156686782836914\n",
      "1.697359323501587\n",
      "1.7409825325012207\n",
      "1.7262077331542969\n",
      "1.7172696590423584\n",
      "1.7378478050231934\n",
      "1.738219976425171\n",
      "1.7737691402435303\n",
      "1.7573866844177246\n",
      "1.7899208068847656\n",
      "1.737257957458496\n",
      "1.7266933917999268\n",
      "1.7467353343963623\n",
      "1.800286054611206\n",
      "1.8301587104797363\n",
      "1.8140313625335693\n",
      "1.815412998199463\n",
      "1.8386774063110352\n",
      "1.7502481937408447\n",
      "1.7546498775482178\n",
      "1.7216439247131348\n",
      "1.7139520645141602\n",
      "1.734102725982666\n",
      "1.766730785369873\n",
      "1.7148945331573486\n",
      "1.7833106517791748\n",
      "1.7734005451202393\n",
      "1.70585036277771\n",
      "1.724942922592163\n",
      "1.7401533126831055\n",
      "1.786569356918335\n",
      "1.7819249629974365\n",
      "1.7671520709991455\n",
      "1.7735843658447266\n",
      "1.746333122253418\n",
      "1.7385661602020264\n",
      "1.7295353412628174\n",
      "1.7467992305755615\n",
      "1.7439870834350586\n",
      "1.7378766536712646\n",
      "1.727947473526001\n",
      "1.7261247634887695\n",
      "1.7668886184692383\n",
      "1.757965087890625\n",
      "1.7098956108093262\n",
      "1.7161567211151123\n",
      "1.7475473880767822\n",
      "1.7373738288879395\n",
      "1.7240145206451416\n",
      "1.7707812786102295\n",
      "1.742889165878296\n",
      "1.7462713718414307\n",
      "1.71854567527771\n",
      "1.710209608078003\n",
      "1.73008394241333\n",
      "1.7021031379699707\n",
      "1.7788960933685303\n",
      "1.7450900077819824\n",
      "1.8584015369415283\n",
      "1.7332096099853516\n",
      "1.7535181045532227\n",
      "1.7345988750457764\n",
      "1.705899953842163\n",
      "1.7217652797698975\n",
      "1.743865966796875\n",
      "1.8216118812561035\n",
      "1.7697272300720215\n",
      "1.7317442893981934\n",
      "1.7115681171417236\n",
      "1.6966917514801025\n",
      "1.707679271697998\n",
      "1.7149684429168701\n",
      "1.8122317790985107\n",
      "1.7482752799987793\n",
      "1.8094210624694824\n",
      "1.7639541625976562\n",
      "1.7625219821929932\n",
      "1.741753339767456\n",
      "1.8320701122283936\n",
      "1.7860188484191895\n",
      "1.7370426654815674\n",
      "1.7107844352722168\n",
      "1.7204759120941162\n",
      "1.7009966373443604\n",
      "1.6947500705718994\n",
      "1.695286512374878\n",
      "1.78989577293396\n",
      "1.7789397239685059\n",
      "1.760901927947998\n",
      "1.7698473930358887\n",
      "1.810772180557251\n",
      "1.8855869770050049\n",
      "1.7996716499328613\n",
      "1.7481536865234375\n",
      "1.766385555267334\n",
      "1.7444953918457031\n",
      "1.760077714920044\n",
      "1.7388951778411865\n",
      "1.7563433647155762\n",
      "1.8062007427215576\n",
      "1.7965025901794434\n",
      "1.8165283203125\n",
      "1.7963061332702637\n",
      "1.7573270797729492\n",
      "1.7477848529815674\n",
      "1.7837724685668945\n",
      "1.8229014873504639\n",
      "1.7361719608306885\n",
      "1.7474637031555176\n",
      "1.7330670356750488\n",
      "1.7600791454315186\n",
      "1.8728973865509033\n",
      "1.8077125549316406\n",
      "1.7992825508117676\n",
      "1.770296573638916\n",
      "1.714850664138794\n",
      "1.8134026527404785\n",
      "1.791583776473999\n",
      "1.7790043354034424\n",
      "1.8016371726989746\n",
      "1.8353991508483887\n",
      "1.7997636795043945\n",
      "1.72995924949646\n",
      "1.8057479858398438\n",
      "1.817530870437622\n",
      "1.7633898258209229\n",
      "1.8085441589355469\n",
      "1.8165063858032227\n",
      "1.7654013633728027\n",
      "1.801649570465088\n",
      "1.7638633251190186\n",
      "1.816061019897461\n",
      "1.8156063556671143\n",
      "1.8126347064971924\n",
      "1.7361254692077637\n",
      "1.7247416973114014\n",
      "1.7163500785827637\n",
      "1.7542738914489746\n",
      "1.702338695526123\n",
      "1.7369065284729004\n",
      "1.7870914936065674\n",
      "1.7331840991973877\n",
      "1.7267940044403076\n",
      "1.780733585357666\n",
      "1.7640841007232666\n",
      "1.8043837547302246\n",
      "1.7778124809265137\n",
      "1.7766294479370117\n",
      "1.8049163818359375\n",
      "1.7699637413024902\n",
      "1.800410270690918\n",
      "1.7612817287445068\n",
      "1.7350854873657227\n",
      "1.7275912761688232\n",
      "1.7211709022521973\n",
      "1.7538559436798096\n",
      "1.7814626693725586\n",
      "1.7082140445709229\n",
      "1.725332260131836\n",
      "1.775444507598877\n",
      "1.724083662033081\n",
      "1.725266695022583\n",
      "1.7518482208251953\n",
      "1.7424957752227783\n",
      "1.7168171405792236\n",
      "1.855583667755127\n",
      "1.7912428379058838\n",
      "1.782289981842041\n",
      "1.8231892585754395\n",
      "1.7469770908355713\n",
      "1.7751893997192383\n",
      "1.8348734378814697\n",
      "1.8046655654907227\n",
      "1.7249209880828857\n",
      "1.784120798110962\n",
      "1.7381031513214111\n",
      "1.68412446975708\n",
      "1.6952862739562988\n",
      "1.7068579196929932\n",
      "1.7266499996185303\n",
      "1.7692992687225342\n",
      "1.751842737197876\n",
      "1.741755485534668\n",
      "1.7054996490478516\n",
      "1.7020673751831055\n",
      "1.7309939861297607\n",
      "1.749403715133667\n",
      "1.7730400562286377\n",
      "1.7660839557647705\n",
      "1.7181994915008545\n",
      "1.7352216243743896\n",
      "1.8211860656738281\n",
      "1.794060230255127\n",
      "1.8413505554199219\n",
      "1.6927847862243652\n",
      "1.7050089836120605\n",
      "1.7164185047149658\n",
      "1.7544801235198975\n",
      "1.722059965133667\n",
      "1.7496435642242432\n",
      "1.7001440525054932\n",
      "1.7990505695343018\n",
      "1.699146032333374\n",
      "1.7687304019927979\n",
      "1.724780559539795\n",
      "1.7168867588043213\n",
      "1.7744784355163574\n",
      "1.690192699432373\n"
     ]
    }
   ],
   "source": [
    "imag = np.zeros((256,256))\n",
    "armonico = 3 \n",
    "output = forward(imag, armonico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.local/lib/python3.10/site-packages/torch/_tensor.py:958: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return self.numpy().astype(dtype, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.374554634094238\n",
      "5.407871246337891\n",
      "5.667499542236328\n",
      "5.453071117401123\n",
      "5.477507591247559\n",
      "5.697525978088379\n",
      "5.594484806060791\n",
      "5.497110843658447\n",
      "5.500065326690674\n",
      "5.684385776519775\n",
      "5.771633148193359\n",
      "5.752591371536255\n",
      "5.430881500244141\n",
      "5.328273057937622\n",
      "5.419217586517334\n",
      "5.322933673858643\n",
      "5.454589366912842\n",
      "5.500595331192017\n",
      "5.693443059921265\n",
      "5.772945404052734\n",
      "5.586004257202148\n",
      "5.411581993103027\n",
      "5.455182313919067\n",
      "5.366280794143677\n",
      "5.458601951599121\n",
      "5.498173713684082\n",
      "5.438470125198364\n",
      "5.331056118011475\n",
      "5.408310651779175\n",
      "5.431191444396973\n",
      "5.179216146469116\n",
      "5.31455659866333\n",
      "5.417023658752441\n",
      "5.595264196395874\n",
      "5.369808673858643\n",
      "5.1485676765441895\n",
      "5.158420085906982\n",
      "5.15882134437561\n",
      "5.345545530319214\n",
      "5.1473000049591064\n",
      "5.138144493103027\n",
      "5.177318572998047\n",
      "5.2596142292022705\n",
      "5.3678319454193115\n",
      "5.21525239944458\n",
      "5.188101530075073\n",
      "5.128096103668213\n",
      "5.285202503204346\n",
      "5.457069635391235\n",
      "5.17292332649231\n",
      "5.194209814071655\n",
      "5.198408126831055\n",
      "5.170896768569946\n",
      "5.202244520187378\n",
      "5.171592473983765\n",
      "5.183793067932129\n",
      "5.300189018249512\n",
      "5.340508222579956\n",
      "5.193986654281616\n",
      "5.14422082901001\n",
      "5.190591812133789\n",
      "5.200943946838379\n",
      "5.164695739746094\n",
      "5.166761636734009\n",
      "5.18731164932251\n",
      "5.368642330169678\n",
      "5.4835450649261475\n",
      "5.532774209976196\n",
      "5.4459874629974365\n",
      "5.406622409820557\n",
      "5.4226906299591064\n",
      "5.6450135707855225\n",
      "5.490375757217407\n",
      "5.408515930175781\n",
      "5.551290035247803\n",
      "5.405427694320679\n",
      "5.344122409820557\n",
      "5.357948303222656\n",
      "5.512014627456665\n",
      "5.511949777603149\n",
      "5.349469423294067\n",
      "5.470818281173706\n",
      "5.375415802001953\n",
      "5.57050895690918\n",
      "5.536038398742676\n",
      "5.699600696563721\n",
      "5.48953652381897\n",
      "5.4392476081848145\n",
      "5.577866792678833\n",
      "5.367857217788696\n",
      "5.396575689315796\n",
      "5.363165616989136\n",
      "5.3757593631744385\n",
      "5.377498149871826\n",
      "5.463418960571289\n",
      "5.46898078918457\n",
      "5.485647916793823\n",
      "5.329357862472534\n",
      "5.385349273681641\n",
      "5.700493574142456\n",
      "5.3333189487457275\n",
      "5.46188497543335\n",
      "5.347177028656006\n",
      "5.307949066162109\n",
      "5.363908529281616\n",
      "5.360008716583252\n",
      "5.3772172927856445\n",
      "5.307333707809448\n",
      "5.342708110809326\n",
      "5.371925592422485\n",
      "5.348988771438599\n",
      "5.42573618888855\n",
      "5.3348164558410645\n",
      "5.3554792404174805\n",
      "5.3449928760528564\n",
      "5.397494792938232\n",
      "5.375056266784668\n",
      "5.437518835067749\n",
      "5.306289196014404\n",
      "5.339807510375977\n",
      "5.367459774017334\n",
      "5.291894197463989\n",
      "5.475182294845581\n",
      "5.493931531906128\n",
      "5.311734199523926\n",
      "5.62263822555542\n",
      "5.605544090270996\n",
      "5.412374496459961\n",
      "5.344976902008057\n",
      "5.52335786819458\n",
      "5.626388311386108\n",
      "5.546247482299805\n",
      "5.52060079574585\n",
      "5.527101516723633\n",
      "5.7399749755859375\n",
      "5.442463159561157\n",
      "5.617402791976929\n",
      "5.716656446456909\n",
      "5.580766677856445\n",
      "5.499269247055054\n",
      "5.628371715545654\n",
      "5.538939952850342\n",
      "5.540590286254883\n",
      "5.509627342224121\n",
      "5.367084264755249\n",
      "5.423705577850342\n",
      "5.645920753479004\n",
      "5.750321865081787\n",
      "5.554253816604614\n",
      "5.642115354537964\n",
      "5.510737895965576\n",
      "5.618008613586426\n",
      "5.634877681732178\n",
      "5.4276511669158936\n",
      "5.351204872131348\n",
      "5.406845331192017\n",
      "5.342286586761475\n",
      "5.343433856964111\n",
      "5.334691524505615\n",
      "5.518772840499878\n",
      "5.438482046127319\n",
      "5.4338014125823975\n",
      "5.3552117347717285\n",
      "5.352205991744995\n",
      "5.333739995956421\n",
      "5.336529493331909\n",
      "5.36837363243103\n",
      "5.419169187545776\n",
      "5.442363500595093\n",
      "5.4059648513793945\n",
      "5.365126609802246\n",
      "5.554931640625\n",
      "5.648841857910156\n",
      "5.35310959815979\n",
      "5.3634934425354\n",
      "5.3432536125183105\n",
      "5.354689359664917\n",
      "5.325658321380615\n",
      "5.459968090057373\n",
      "5.442726373672485\n",
      "5.3187150955200195\n",
      "5.413167238235474\n",
      "5.676575660705566\n",
      "5.460223436355591\n",
      "5.40113377571106\n",
      "5.465123653411865\n",
      "5.439322471618652\n",
      "5.43259859085083\n",
      "5.314038276672363\n",
      "5.430032968521118\n",
      "5.458605527877808\n",
      "5.418651342391968\n",
      "5.531919240951538\n",
      "5.348814487457275\n",
      "5.3450608253479\n",
      "5.388898849487305\n",
      "5.3408942222595215\n",
      "5.411503076553345\n",
      "5.517924070358276\n",
      "5.602924346923828\n",
      "5.37363076210022\n",
      "5.395629405975342\n",
      "5.398426532745361\n",
      "5.4396302700042725\n",
      "5.460201263427734\n",
      "5.5154595375061035\n",
      "5.3686699867248535\n",
      "5.329472541809082\n",
      "5.478075981140137\n",
      "5.372747421264648\n",
      "5.42444920539856\n",
      "5.506157159805298\n",
      "5.452499151229858\n",
      "5.464025020599365\n",
      "5.387604475021362\n",
      "5.336390495300293\n",
      "5.373301267623901\n",
      "5.523259878158569\n",
      "5.6139819622039795\n",
      "5.558601379394531\n",
      "5.352986097335815\n",
      "5.4242589473724365\n",
      "5.483555316925049\n",
      "5.389683723449707\n",
      "5.720087051391602\n",
      "5.4960691928863525\n",
      "5.544175386428833\n",
      "5.58451509475708\n",
      "5.4512364864349365\n",
      "5.404116153717041\n",
      "5.4362077713012695\n",
      "5.521065950393677\n",
      "5.563701868057251\n",
      "5.478046417236328\n",
      "5.603951454162598\n",
      "5.4402735233306885\n",
      "5.3737382888793945\n",
      "5.484564304351807\n",
      "5.377944231033325\n",
      "5.429362058639526\n",
      "5.625797748565674\n",
      "5.668793201446533\n",
      "5.522244930267334\n",
      "5.683594703674316\n",
      "5.464501619338989\n",
      "5.682199716567993\n",
      "5.627281665802002\n",
      "5.4501049518585205\n",
      "5.588555335998535\n",
      "5.626544952392578\n",
      "5.509299039840698\n",
      "5.365896463394165\n",
      "5.512186288833618\n",
      "5.4915876388549805\n",
      "5.6144020557403564\n",
      "5.627554416656494\n",
      "5.6058855056762695\n",
      "5.628575563430786\n",
      "5.4362640380859375\n",
      "5.429345369338989\n",
      "5.705954313278198\n",
      "5.6460466384887695\n",
      "5.71054220199585\n",
      "5.415466070175171\n",
      "5.684432506561279\n",
      "5.663290977478027\n",
      "5.715727806091309\n",
      "5.436629772186279\n",
      "5.707615852355957\n",
      "5.545007705688477\n",
      "5.502763748168945\n",
      "5.473177909851074\n",
      "5.231688976287842\n",
      "5.216555595397949\n",
      "5.1980140209198\n",
      "5.2905356884002686\n",
      "5.248252868652344\n",
      "5.247854948043823\n",
      "5.389350175857544\n",
      "5.292267322540283\n",
      "5.240479946136475\n",
      "5.362127065658569\n",
      "5.278772354125977\n",
      "5.31312894821167\n",
      "5.262857675552368\n",
      "5.285700798034668\n",
      "5.225410461425781\n",
      "5.587145566940308\n",
      "5.523658990859985\n",
      "5.355082988739014\n",
      "5.261114120483398\n",
      "5.202188014984131\n",
      "5.339541912078857\n",
      "5.331629276275635\n",
      "5.4492480754852295\n",
      "5.751892566680908\n",
      "5.70782470703125\n",
      "5.63318943977356\n",
      "5.205199241638184\n",
      "5.288346529006958\n",
      "5.270325183868408\n",
      "5.2123565673828125\n",
      "5.249830007553101\n",
      "5.252776861190796\n",
      "5.294016361236572\n",
      "5.200043439865112\n",
      "5.303454160690308\n",
      "5.264776945114136\n",
      "5.209877252578735\n",
      "5.255773544311523\n",
      "5.314498662948608\n",
      "5.273346900939941\n",
      "5.227022171020508\n",
      "5.16493821144104\n",
      "5.285372972488403\n",
      "5.361544609069824\n",
      "5.280251741409302\n",
      "5.342185020446777\n",
      "5.336865186691284\n",
      "5.306830644607544\n",
      "5.27631402015686\n",
      "5.2926247119903564\n",
      "5.232142210006714\n",
      "5.315487623214722\n",
      "5.253692865371704\n",
      "5.392558813095093\n",
      "5.529017686843872\n",
      "5.472665309906006\n",
      "5.395013332366943\n",
      "5.427119255065918\n",
      "5.549124002456665\n",
      "5.5596089363098145\n",
      "5.81692361831665\n",
      "5.593693256378174\n",
      "5.4493629932403564\n",
      "5.592407941818237\n",
      "5.6222310066223145\n",
      "5.855520009994507\n",
      "5.707676649093628\n",
      "5.67123556137085\n",
      "5.720724582672119\n",
      "5.616315603256226\n",
      "5.711685419082642\n",
      "5.619516372680664\n",
      "5.630860328674316\n",
      "5.528735637664795\n",
      "5.599853038787842\n",
      "5.409485340118408\n",
      "5.258136987686157\n",
      "5.434001684188843\n",
      "5.15023946762085\n",
      "5.38005256652832\n",
      "5.498548984527588\n",
      "5.56174898147583\n",
      "5.450133562088013\n",
      "5.2697365283966064\n",
      "5.463598966598511\n",
      "5.580809116363525\n",
      "5.40753173828125\n",
      "5.493841648101807\n",
      "5.349017381668091\n",
      "5.287446022033691\n",
      "5.182579040527344\n",
      "5.193929433822632\n",
      "5.154296636581421\n",
      "5.252052068710327\n",
      "5.213494539260864\n",
      "5.172333002090454\n",
      "5.156722545623779\n",
      "5.150362014770508\n",
      "5.141479015350342\n",
      "5.1519598960876465\n",
      "5.137635707855225\n",
      "5.144294500350952\n",
      "5.158572673797607\n",
      "5.147627115249634\n",
      "5.141865491867065\n",
      "5.172213792800903\n",
      "5.205789804458618\n",
      "5.145516395568848\n",
      "5.160263538360596\n",
      "5.15217661857605\n",
      "5.149118185043335\n",
      "5.14684271812439\n",
      "5.166750192642212\n",
      "5.139714241027832\n",
      "5.16283392906189\n",
      "5.145373821258545\n",
      "5.150921821594238\n",
      "5.1566526889801025\n",
      "5.149645090103149\n",
      "5.149576902389526\n",
      "5.1446144580841064\n",
      "5.138935804367065\n",
      "5.187998294830322\n",
      "5.150296211242676\n",
      "5.149876117706299\n",
      "5.151404619216919\n",
      "5.159889459609985\n",
      "5.14816951751709\n",
      "5.158586740493774\n",
      "5.156394720077515\n",
      "5.156439542770386\n",
      "5.242837429046631\n",
      "5.162915468215942\n",
      "5.142590045928955\n",
      "5.182744264602661\n",
      "5.159970045089722\n",
      "5.214064359664917\n",
      "5.160562992095947\n",
      "5.236256837844849\n",
      "5.1630942821502686\n",
      "5.1818530559539795\n",
      "5.159466505050659\n",
      "5.16132926940918\n",
      "5.171202182769775\n",
      "5.182399034500122\n",
      "5.154861211776733\n",
      "5.157144069671631\n",
      "5.243864297866821\n",
      "5.160837650299072\n",
      "5.170475959777832\n",
      "5.176570177078247\n",
      "5.181068658828735\n",
      "5.218396186828613\n",
      "5.244042634963989\n",
      "5.25862979888916\n",
      "5.183406829833984\n",
      "5.227120399475098\n",
      "5.249565124511719\n",
      "5.187314510345459\n",
      "5.160768270492554\n",
      "5.257038354873657\n",
      "5.167015314102173\n",
      "5.223618745803833\n",
      "5.1794538497924805\n",
      "5.167796850204468\n",
      "5.155678749084473\n",
      "5.1599719524383545\n",
      "5.1712119579315186\n",
      "5.159757852554321\n",
      "5.1671364307403564\n",
      "5.19947361946106\n",
      "5.209937334060669\n",
      "5.189114093780518\n",
      "5.200927495956421\n",
      "5.194661855697632\n",
      "5.172013282775879\n",
      "5.189729452133179\n",
      "5.169852256774902\n",
      "5.202621936798096\n",
      "5.166008472442627\n",
      "5.178576469421387\n",
      "5.18848443031311\n",
      "5.176417112350464\n",
      "5.2248854637146\n",
      "5.252570390701294\n",
      "5.245194911956787\n",
      "5.1927995681762695\n",
      "5.222528457641602\n",
      "5.1751885414123535\n",
      "5.257034540176392\n",
      "5.220561981201172\n",
      "5.2513792514801025\n",
      "5.1818687915802\n",
      "5.161566972732544\n",
      "5.194412708282471\n",
      "5.163896799087524\n",
      "5.146698713302612\n",
      "5.155255079269409\n",
      "5.150580644607544\n",
      "5.158055067062378\n",
      "5.2195212841033936\n",
      "5.238421201705933\n",
      "5.239817380905151\n",
      "5.189801454544067\n"
     ]
    }
   ],
   "source": [
    "image_path = 'VAMO!!.png'\n",
    "image = np.asarray(Image.open(image_path))[:,:,0]\n",
    "armonico = 3 \n",
    "output = forward(image, armonico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_sim = output\n",
    "plt.figure()\n",
    "plt.imshow(np.abs((img_sim - img_sim.min())/(img_sim.max()-img_sim.min())),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_save = 255*np.abs((img_sim - img_sim.min())/(img_sim.max()-img_sim.min()))\n",
    "im = Image.fromarray(I_save.astype('uint8'))\n",
    "im.save('zeros_simulado.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
