{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be tensor with float (each bit)\n",
    "def IndA_diff(input):\n",
    "    weights = torch.tensor([[-7.8433e-04, 37.006, 37.006, 37.006, 37.005, 37.006, 37.006, 37.007]],dtype=torch.float64)\n",
    "    bias = torch.tensor([-131.3873],dtype=torch.float64)\n",
    "    linear_output = torch.matmul(input, weights.t()) + bias\n",
    "    output = torch.sigmoid(linear_output)\n",
    "\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo de regresión logística1\n",
    "class LogisticRegressionB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionB, self).__init__()\n",
    "        self.linear1 = nn.Linear(9, 20,dtype=torch.float64) \n",
    "        self.linear2 = nn.Linear(20, 10,dtype=torch.float64)\n",
    "        self.linear3 = nn.Linear(10, 1,dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo de regresión logística\n",
    "class LogisticRegressionD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionD, self).__init__()\n",
    "        self.linear1 = nn.Linear(9, 10,dtype=torch.float64) # 9 entradas y 20 salida\n",
    "        self.linear2 = nn.Linear(10, 5,dtype=torch.float64)\n",
    "        self.linear3 = nn.Linear(5, 1,dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndB = LogisticRegressionB()\n",
    "state_dictB = torch.load('indicatrizB.pth')\n",
    "IndB.load_state_dict(state_dictB)\n",
    "#input must be tensor with float (each bit)\n",
    "def IndB_diff(bits):\n",
    "    return IndB(bits).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 60,dtype=torch.float64)  \n",
    "        self.fc2 = nn.Linear(60, 30,dtype=torch.float64)\n",
    "        self.fc3 = nn.Linear(30, 20,dtype=torch.float64)\n",
    "        self.fc4 = nn.Linear(20, 1,dtype=torch.float64)    \n",
    "        self.sigmoid = nn.Sigmoid()  # Función de activación sigmoide\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndE = Net()\n",
    "state_dictE = torch.load('indicatrizE.pth')\n",
    "IndE.load_state_dict(state_dictE)\n",
    "#bits must be tensor and cnt integer\n",
    "def IndE_diff(bits,cnt):\n",
    "    cnt = torch.tensor([cnt],dtype=torch.float64)\n",
    "    input = torch.cat((bits,cnt))\n",
    "    return IndE(input).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndC = Net()\n",
    "state_dictC = torch.load('indicatrizC.pth')\n",
    "IndC.load_state_dict(state_dictC)\n",
    "#bits must be tensor and cnt integer\n",
    "def IndC_diff(bits,cnt):\n",
    "    cnt = torch.tensor([cnt],dtype=torch.float64)\n",
    "    input = torch.cat((bits,cnt))\n",
    "    return IndC(input).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndD = LogisticRegressionD()\n",
    "state_dictD = torch.load('indicatrizD.pth')\n",
    "IndD.load_state_dict(state_dictD)\n",
    "#input must be tensor with float (each bit)\n",
    "def IndD_diff(bits):\n",
    "    return IndD(bits).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegBit_diff(x):\n",
    "    return 1 / (1 + np.exp(-(-15.43*x + 7.51)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Real2Bit_diff(x):\n",
    "    return 1 / (1 + np.exp(-(-5.43 + 10.81*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XNOR_diff(x1, x2):\n",
    "    fc1_weight = torch.tensor([[4.7662,  4.8464],[-0.1215,  1.3168],[ 2.6322,  2.3068],[-0.0951, -0.4683]],dtype=torch.float64)\n",
    "    fc1_bias = torch.tensor([-1.5654,  0.3904, -3.6841, -0.0062],dtype=torch.float64)\n",
    "    fc2_weight = torch.tensor([[-5.6381,  1.5594,  5.0998, -0.2311]],dtype=torch.float64)\n",
    "    fc2_bias = torch.tensor([1.7433],dtype=torch.float64)\n",
    "\n",
    "    x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    fc1_output = torch.sigmoid(torch.matmul(x, fc1_weight.transpose(0, 1)) + fc1_bias)\n",
    "    fc2_output = torch.sigmoid(torch.matmul(fc1_output, fc2_weight.transpose(0, 1)) + fc2_bias)\n",
    "\n",
    "    return fc2_output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_diff(x1, x2):\n",
    "    fc1_weight = torch.tensor([[-3.8973,  4.0330],[-4.1031,  3.8928],[-0.6775,  0.8108],[ 2.3834, -2.4550]],dtype=torch.float64)\n",
    "    fc1_bias = torch.tensor([ 2.2917, -2.1209, -0.6751, -1.6954],dtype=torch.float64)\n",
    "    fc2_weight = torch.tensor([[-4.5110,  5.7411,  1.0059,  2.4664]],dtype=torch.float64)\n",
    "    fc2_bias = torch.tensor([0.8839],dtype=torch.float64)\n",
    "\n",
    "    x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    fc1_output = torch.sigmoid(torch.matmul(x, fc1_weight.transpose(0, 1)) + fc1_bias)\n",
    "    fc2_output = torch.sigmoid(torch.matmul(fc1_output, fc2_weight.transpose(0, 1)) + fc2_bias)\n",
    "\n",
    "    return fc2_output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_diff(x1, x2):\n",
    "    weight = torch.tensor([[13.5223, 13.5223]],dtype=torch.float64)\n",
    "    bias = torch.tensor([-6.3024],dtype=torch.float64)\n",
    "    x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    output = torch.sigmoid(torch.matmul(x, weight.transpose(0, 1)) + bias)\n",
    "\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit)\n",
    "def q_m_diff(input):\n",
    "    input_cp = input.copy()\n",
    "    t_input = torch.tensor(input_cp,dtype=torch.float64)\n",
    "    output = torch.tensor([t_input[0], 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float64)\n",
    "    for bit in range(1, 8):\n",
    "        output[bit] = IndA_diff(t_input) * Real2Bit_diff(XNOR_diff(output[bit-1], t_input[bit])) + (1 - IndA_diff(t_input)) * Real2Bit_diff(XOR_diff(output[bit-1],t_input[bit]))\n",
    "    output[8] = 1 - IndA_diff(t_input)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit) example: [0.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0], 2.0\n",
    "#output are tensors\n",
    "def TMDS_diff(pixel_bits,cnt):\n",
    "    bits_inversos = pixel_bits[::-1]\n",
    "    q_m = q_m_diff(bits_inversos)\n",
    "    q_m = Real2Bit_diff(q_m)\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float64)\n",
    "    for bit in range(0,8):\n",
    "        output[bit] = q_m[bit] * OR_diff(IndE_diff(q_m[:8],cnt) * IndB_diff(q_m[:9]),(1-IndC_diff(q_m[:8],cnt)) * (1-IndE_diff(q_m[:8],cnt))) + NegBit_diff(q_m[bit]) * OR_diff((1 - IndB_diff(q_m[:9])) * IndE_diff(q_m[:8],cnt),IndC_diff(q_m[:8],cnt) * (1 - IndE_diff(q_m[:8],cnt)))\n",
    "    output[8] = q_m[8]\n",
    "    output[9] = IndE_diff(q_m[:8],cnt) * NegBit_diff(q_m[8]) + (1 - IndE_diff(q_m[:8],cnt)) * IndC_diff(q_m[:8],cnt)\n",
    "    new_cnt = IndE_diff(q_m[:8],cnt) * ((cnt + torch.sum(q_m[:8] < 0.5).item() - torch.sum(q_m[:8] > 0.5).item()) * IndD_diff(q_m) + (cnt + torch.sum(q_m[:8] > 0.5).item() - torch.sum(q_m[:8] < 0.5).item()) * (1 - IndD_diff(q_m))) + (1 - IndE_diff(q_m[:8],cnt)) * ((cnt + 2 * q_m[8] + torch.sum(q_m[:8] < 0.5).item() - torch.sum(q_m[:8] > 0.5).item()) * IndC_diff(q_m[:8],cnt) + (cnt - 2 * NegBit_diff(q_m[8]) + torch.sum(q_m[:8] > 0.5).item() - torch.sum(q_m[:8] < 0.5).item()) * (1 - IndC_diff(q_m[:8],cnt)))\n",
    "    return output,new_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    if x >= 0:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x) / (1 + np.exp(x))\n",
    "\n",
    "def Pixel2Bit_diff(pixel):\n",
    "    output = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(1,9):\n",
    "        output[i-1] = sigmoid(10*(pixel-2**(8-i)+0.5))  # 0.5 para ajustar la sigmoidal\n",
    "        if pixel >= 2**(8-i):\n",
    "            pixel = pixel - 2**(8-i)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funcion que toma como entrada el armonico a sintonizar y las dimensiones de la imagen a espiar y devuelve un array con taps de g(t)\n",
    "def g_taps(dim_vertical, dim_horizontal, armonico):\n",
    "\n",
    "    #defino variables iniciales\n",
    "    f_b = 10 * (dim_vertical * dim_horizontal * 60)\n",
    "    f_sdr = 50e6\n",
    "    harm = armonico * f_b\n",
    "    \n",
    "    #para el correcto funcionamiento: dependiendo del armonico, elijo cuantas muestras por pulso\n",
    "    if (armonico < 5 ):\n",
    "        muestras_por_pulso  = 10\n",
    "    else:\n",
    "        muestras_por_pulso  = 20\n",
    "\n",
    "    samp_rate = muestras_por_pulso * f_b\n",
    "    H_samples = dim_horizontal * muestras_por_pulso\n",
    "\n",
    "    #creo el pulso\n",
    "    t_continuous = np.linspace(start = 0, stop = H_samples/samp_rate, num = H_samples, endpoint= False)\n",
    "    pulso = np.zeros(H_samples)\n",
    "    pulso[:muestras_por_pulso] = 0.7/255\n",
    "\n",
    "    #traslado el espectro del pulso el armonico correspondiente\n",
    "    frec_armonico = np.exp(-2j*np.pi*harm*t_continuous)\n",
    "    pulso_complejo = pulso*frec_armonico\n",
    "\n",
    "    #creo el lpf del sdr\n",
    "    b, a = signal.butter(6, f_sdr/2, fs=samp_rate, btype='lowpass', analog=False)\n",
    "\n",
    "    #filtro con lpf el pulso multiplicado por armonico. El resultado es g\n",
    "    g_t = signal.lfilter(b, a, pulso_complejo)\n",
    "\n",
    "    # si arminico crece, necesito mas taps\n",
    "    if (armonico < 5):\n",
    "        g_t = g_t[:1500]\n",
    "    else:\n",
    "        g_t = g_t[:3000]\n",
    "\n",
    "    return torch.tensor(g_t).reshape(1,1,len(g_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(img, armonico):\n",
    "    filas, columnas = img.shape\n",
    "    img_salida = img.copy()\n",
    "    bits_codificados_fila = torch.zeros((1,1,10*columnas),dtype = torch.complex64)\n",
    "    g_t = g_taps(filas, columnas, armonico)\n",
    "    size_g_t = g_t.numel()\n",
    "    for i in range(filas):\n",
    "        cnt = 0\n",
    "        for j in range(columnas):\n",
    "            pixel = img[i,j]\n",
    "            pixel_bits = Pixel2Bit_diff(pixel)\n",
    "            bits_cods, cnt = TMDS_diff(pixel_bits, cnt)\n",
    "            bits_codificados_fila[0,0,j*10:(j+1)*10] = bits_cods\n",
    "       # cadena_bits = torch.tensor(cadena_bits)\n",
    "        #calculo y[n] en la linea horizontal i\n",
    "        padding = (size_g_t - 8)//2\n",
    "        img_salida[i,:] = nn.functional.conv1d(bits_codificados_fila,g_t,stride = 10, padding = padding, bias = None)\n",
    "        #convolucion = torch.nn.Conv1d(1, 1, kernel_size = size_g_t, stride = 10, padding = padding, bias = False, dtype = torch.complex64)\n",
    "        #convolucion.weight.data = g_t\n",
    "        #img_salida[i,:] = convolucion\n",
    "    return img_salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found ComplexDouble",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m imag \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m1080\u001b[39m,\u001b[39m1920\u001b[39m))\n\u001b[1;32m      2\u001b[0m armonico \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \n\u001b[0;32m----> 3\u001b[0m output \u001b[39m=\u001b[39m forward(imag, armonico)\n",
      "Cell \u001b[0;32mIn[75], line 17\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(img, armonico)\u001b[0m\n\u001b[1;32m     14\u001b[0m    \u001b[39m# cadena_bits = torch.tensor(cadena_bits)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m#calculo y[n] en la linea horizontal i\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     padding \u001b[39m=\u001b[39m (size_g_t \u001b[39m-\u001b[39m \u001b[39m8\u001b[39m)\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n\u001b[0;32m---> 17\u001b[0m     img_salida[i,:] \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mconv1d(bits_codificados_fila,g_t,stride \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, padding \u001b[39m=\u001b[39;49m padding, bias \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m     18\u001b[0m     \u001b[39m#convolucion = torch.nn.Conv1d(1, 1, kernel_size = size_g_t, stride = 10, padding = padding, bias = False, dtype = torch.complex64)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[39m#convolucion.weight.data = g_t\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m#img_salida[i,:] = convolucion\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39mreturn\u001b[39;00m img_salida\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found ComplexDouble"
     ]
    }
   ],
   "source": [
    "imag = np.zeros((1080,1920))\n",
    "armonico = 3 \n",
    "output = forward(imag, armonico)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
