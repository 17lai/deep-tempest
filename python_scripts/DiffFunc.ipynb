{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be tensor with float (each bit)\n",
    "def IndA_diff(input):\n",
    "    weights = torch.tensor([[-7.8433e-04, 37.006, 37.006, 37.006, 37.005, 37.006, 37.006, 37.007]],dtype=torch.float64)\n",
    "    bias = torch.tensor([-131.3873],dtype=torch.float64)\n",
    "    linear_output = torch.matmul(input, weights.t()) + bias\n",
    "    output = torch.sigmoid(linear_output)\n",
    "\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo de regresión logística1\n",
    "class LogisticRegressionB(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionB, self).__init__()\n",
    "        self.linear1 = nn.Linear(9, 20,dtype=torch.float64) \n",
    "        self.linear2 = nn.Linear(20, 10,dtype=torch.float64)\n",
    "        self.linear3 = nn.Linear(10, 1,dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo de regresión logística\n",
    "class LogisticRegressionD(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionD, self).__init__()\n",
    "        self.linear1 = nn.Linear(9, 10,dtype=torch.float64) # 9 entradas y 20 salida\n",
    "        self.linear2 = nn.Linear(10, 5,dtype=torch.float64)\n",
    "        self.linear3 = nn.Linear(5, 1,dtype=torch.float64)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        x = torch.sigmoid(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndB = LogisticRegressionB()\n",
    "state_dictB = torch.load('indicatrizB.pth')\n",
    "IndB.load_state_dict(state_dictB)\n",
    "#input must be tensor with float (each bit)\n",
    "def IndB_diff(bits):\n",
    "    return IndB(bits).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(9, 60,dtype=torch.float64)  \n",
    "        self.fc2 = nn.Linear(60, 30,dtype=torch.float64)\n",
    "        self.fc3 = nn.Linear(30, 20,dtype=torch.float64)\n",
    "        self.fc4 = nn.Linear(20, 1,dtype=torch.float64)    \n",
    "        self.sigmoid = nn.Sigmoid()  # Función de activación sigmoide\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndE = Net()\n",
    "state_dictE = torch.load('indicatrizE.pth')\n",
    "IndE.load_state_dict(state_dictE)\n",
    "#bits must be tensor and cnt integer\n",
    "def IndE_diff(bits,cnt):\n",
    "    cnt = torch.tensor([cnt],dtype=torch.float64)\n",
    "    input = torch.cat((bits,cnt))\n",
    "    return IndE(input).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndC = Net()\n",
    "state_dictC = torch.load('indicatrizC.pth')\n",
    "IndC.load_state_dict(state_dictC)\n",
    "#bits must be tensor and cnt integer\n",
    "def IndC_diff(bits,cnt):\n",
    "    cnt = torch.tensor([cnt],dtype=torch.float64)\n",
    "    input = torch.cat((bits,cnt))\n",
    "    return IndC(input).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "IndD = LogisticRegressionD()\n",
    "state_dictD = torch.load('indicatrizD.pth')\n",
    "IndD.load_state_dict(state_dictD)\n",
    "#input must be tensor with float (each bit)\n",
    "def IndD_diff(bits):\n",
    "    return IndD(bits).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegBit_diff(x):\n",
    "    return 1 / (1 + np.exp(-(-15.43*x + 7.51)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Real2Bit_diff(x):\n",
    "    return 1 / (1 + np.exp(-(-5.43 + 10.81*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XNOR_diff(x1, x2):\n",
    "    fc1_weight = torch.tensor([[4.7662,  4.8464],[-0.1215,  1.3168],[ 2.6322,  2.3068],[-0.0951, -0.4683]],dtype=torch.float64)\n",
    "    fc1_bias = torch.tensor([-1.5654,  0.3904, -3.6841, -0.0062],dtype=torch.float64)\n",
    "    fc2_weight = torch.tensor([[-5.6381,  1.5594,  5.0998, -0.2311]],dtype=torch.float64)\n",
    "    fc2_bias = torch.tensor([1.7433],dtype=torch.float64)\n",
    "\n",
    "    x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    fc1_output = torch.sigmoid(torch.matmul(x, fc1_weight.transpose(0, 1)) + fc1_bias)\n",
    "    fc2_output = torch.sigmoid(torch.matmul(fc1_output, fc2_weight.transpose(0, 1)) + fc2_bias)\n",
    "\n",
    "    return fc2_output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_diff(x1, x2):\n",
    "    fc1_weight = torch.tensor([[-3.8973,  4.0330],[-4.1031,  3.8928],[-0.6775,  0.8108],[ 2.3834, -2.4550]],dtype=torch.float64)\n",
    "    fc1_bias = torch.tensor([ 2.2917, -2.1209, -0.6751, -1.6954],dtype=torch.float64)\n",
    "    fc2_weight = torch.tensor([[-4.5110,  5.7411,  1.0059,  2.4664]],dtype=torch.float64)\n",
    "    fc2_bias = torch.tensor([0.8839],dtype=torch.float64)\n",
    "\n",
    "    x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    fc1_output = torch.sigmoid(torch.matmul(x, fc1_weight.transpose(0, 1)) + fc1_bias)\n",
    "    fc2_output = torch.sigmoid(torch.matmul(fc1_output, fc2_weight.transpose(0, 1)) + fc2_bias)\n",
    "\n",
    "    return fc2_output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_diff(x1, x2):\n",
    "    weight = torch.tensor([[13.5223, 13.5223]],dtype=torch.float64)\n",
    "    bias = torch.tensor([-6.3024],dtype=torch.float64)\n",
    "    x = torch.tensor([x1, x2],dtype=torch.float64)\n",
    "    output = torch.sigmoid(torch.matmul(x, weight.transpose(0, 1)) + bias)\n",
    "\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit)\n",
    "def q_m_diff(input):\n",
    "    input_cp = input.copy()\n",
    "    t_input = torch.tensor(input_cp,dtype=torch.float64)\n",
    "    output = torch.tensor([t_input[0], 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float64)\n",
    "    for bit in range(1, 8):\n",
    "        output[bit] = IndA_diff(t_input) * Real2Bit_diff(XNOR_diff(output[bit-1], t_input[bit])) + (1 - IndA_diff(t_input)) * Real2Bit_diff(XOR_diff(output[bit-1],t_input[bit]))\n",
    "    output[8] = 1 - IndA_diff(t_input)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input must be float (each bit) example: [0.0 1.0 1.0 1.0 0.0 0.0 0.0 1.0], 2.0\n",
    "#output are tensors\n",
    "def TMDS_diff(pixel_bits,cnt):\n",
    "    bits_inversos = pixel_bits[::-1]\n",
    "    q_m = q_m_diff(bits_inversos)\n",
    "    q_m = Real2Bit_diff(q_m)\n",
    "    output = torch.tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0],dtype=torch.float64)\n",
    "    for bit in range(0,8):\n",
    "        output[bit] = q_m[bit] * OR_diff(IndE_diff(q_m[:8],cnt) * IndB_diff(q_m[:9]),(1-IndC_diff(q_m[:8],cnt)) * (1-IndE_diff(q_m[:8],cnt))) + NegBit_diff(q_m[bit]) * OR_diff((1 - IndB_diff(q_m[:9])) * IndE_diff(q_m[:8],cnt),IndC_diff(q_m[:8],cnt) * (1 - IndE_diff(q_m[:8],cnt)))\n",
    "    output[8] = q_m[8]\n",
    "    output[9] = IndE_diff(q_m[:8],cnt) * NegBit_diff(q_m[8]) + (1 - IndE_diff(q_m[:8],cnt)) * IndC_diff(q_m[:8],cnt)\n",
    "    new_cnt = IndE_diff(q_m[:8],cnt) * ((cnt + torch.sum(q_m[:8] < 0.5).item() - torch.sum(q_m[:8] > 0.5).item()) * IndD_diff(q_m) + (cnt + torch.sum(q_m[:8] > 0.5).item() - torch.sum(q_m[:8] < 0.5).item()) * (1 - IndD_diff(q_m))) + (1 - IndE_diff(q_m[:8],cnt)) * ((cnt + 2 * q_m[8] + torch.sum(q_m[:8] < 0.5).item() - torch.sum(q_m[:8] > 0.5).item()) * IndC_diff(q_m[:8],cnt) + (cnt - 2 * NegBit_diff(q_m[8]) + torch.sum(q_m[:8] > 0.5).item() - torch.sum(q_m[:8] < 0.5).item()) * (1 - IndC_diff(q_m[:8],cnt)))\n",
    "    return output,new_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_simplified(input):\n",
    "   \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pixel2Bit_diff(pixel):\n",
    "    output = [0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for i in range(1,9):\n",
    "        output[i-1] = 1 / (1 + np.exp(-(11*(pixel-2**(8-i)+0.5))))  #0.5 to adjust for the sigmoid so at 127.5 is at 0.5 and at 128 is already at 1\n",
    "        if pixel >= 2**(8-i):\n",
    "            pixel = pixel - 2**(8-i)\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
